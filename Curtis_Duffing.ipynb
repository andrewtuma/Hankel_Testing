{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Curtis_Duffing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andrewtuma/Hankel_Testing/blob/main/Curtis_Duffing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "uQ06y1k6Wv2M",
        "outputId": "fad37405-68fe-45fd-f5c1-46f6bc20d276"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "    Created by:\n",
        "        Opal Issan\n",
        "\n",
        "    Modified:\n",
        "        17 Nov 2020 - Jay Lago\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# Function Implementations\n",
        "# ==============================================================================\n",
        "def dyn_sys_discrete(lhs, mu=-0.05, lam=-1):\n",
        "    \"\"\" example 1:\n",
        "    ODE =>\n",
        "    dx1/dt = mu*x1\n",
        "    dx2/dt = lam*(x2-x1^2)\n",
        "\n",
        "    By default: mu =-0.05, and lambda = -1.\n",
        "    \"\"\"\n",
        "    rhs = np.zeros(2)\n",
        "    rhs[0] = mu * lhs[0]\n",
        "    rhs[1] = lam * (lhs[1] - (lhs[0]) ** 2.)\n",
        "    return rhs\n",
        "\n",
        "def dyn_sys_pendulum(lhs):\n",
        "    \"\"\" pendulum example:\n",
        "    ODE =>\n",
        "    dx1/dt = x2\n",
        "    dx2/dt = -sin(x1)\n",
        "    \"\"\"\n",
        "    rhs = np.zeros(2)\n",
        "    rhs[0] = lhs[1]\n",
        "    rhs[1] = -np.sin(lhs[0])\n",
        "    return rhs\n",
        "\n",
        "def dyn_sys_fluid(lhs, mu=0.1, omega=1, A=-0.1, lam=10):\n",
        "    \"\"\"fluid flow example:\n",
        "    ODE =>\n",
        "    dx1/dt = mu*x1 - omega*x2 + A*x1*x3\n",
        "    dx2/dt = omega*x1 + mu*x2 + A*x2*x3\n",
        "    dx3/dt = -lam(x3 - x1^2 - x2^2)\n",
        "    \"\"\"\n",
        "    rhs = np.zeros(3)\n",
        "    rhs[0] = mu * lhs[0] - omega * lhs[1] + A * lhs[0] * lhs[2]\n",
        "    rhs[1] = omega * lhs[0] + mu * lhs[1] + A * lhs[1] * lhs[2]\n",
        "    rhs[2] = -lam * (lhs[2] - lhs[0] ** 2 - lhs[1] ** 2)\n",
        "    return rhs\n",
        "\n",
        "def dyn_sys_kdv(lhs, a1=0, c=3):\n",
        "    \"\"\" planar kdv:\n",
        "    dx1/dt = x2\n",
        "    dx2/dt = a1 + c*x1 - 3*x2^2\n",
        "    \"\"\"\n",
        "    rhs = np.zeros(2)\n",
        "    rhs[0] = lhs[1]\n",
        "    rhs[1] = a1 + c*lhs[0] - 3*lhs[0]**2\n",
        "    return rhs\n",
        "\n",
        "def dyn_sys_duffing_driven(lhs, alpha=0.1, gamma=0.05, omega=1.1):\n",
        "    \"\"\" Duffing oscillator:\n",
        "    dx/dt = y\n",
        "    dy/dt = x - x^3 - gamma*y + alpha*cos(omega*t)\n",
        "    \"\"\"\n",
        "    rhs = np.zeros(3)\n",
        "    rhs[0] = lhs[1]\n",
        "    rhs[1] = lhs[0] - lhs[0]**3 - gamma*lhs[1] + alpha*np.cos(omega*lhs[2])\n",
        "    rhs[2] = lhs[2]\n",
        "    return rhs\n",
        "\n",
        "def dyn_sys_duffing(lhs):\n",
        "    \"\"\" Duffing oscillator:\n",
        "    dx/dt = y\n",
        "    dy/dt = x - x^3\n",
        "    \"\"\"\n",
        "    rhs = np.zeros(2)\n",
        "    rhs[0] = lhs[1]\n",
        "    rhs[1] = lhs[0] - lhs[0]**3\n",
        "    return rhs\n",
        "\n",
        "def dyn_sys_duffing_bollt(lhs, alpha=1.0, beta=-1.0, delta=0.5):\n",
        "    \"\"\" Duffing oscillator:\n",
        "    dx/dt = y\n",
        "    dy/dt = -delta*y - x*(beta + alpha*x^2)\n",
        "    \"\"\"\n",
        "    rhs = np.zeros(2)\n",
        "    rhs[0] = lhs[1]\n",
        "    rhs[1] = -delta*lhs[1] - lhs[0]*(beta + alpha*lhs[0]**2)\n",
        "    return rhs\n",
        "\n",
        "def rk4(lhs, dt, function):\n",
        "    \"\"\"\n",
        "    :param lhs: previous step state.\n",
        "    :param dt: delta t.\n",
        "    :param data_type: \"ex1\" or \"ex2\".\n",
        "    :return:  Runge–Kutta 4th order method.\n",
        "    \"\"\"\n",
        "    k1 = dt * function(lhs)\n",
        "    k2 = dt * function(lhs + k1 / 2.0)\n",
        "    k3 = dt * function(lhs + k2 / 2.0)\n",
        "    k4 = dt * function(lhs + k3)\n",
        "    rhs = lhs + 1.0 / 6.0 * (k1 + 2.0 * (k2 + k3) + k4)\n",
        "    return rhs\n",
        "\n",
        "def data_maker_discrete(x_lower1, x_upper1, x_lower2, x_upper2, n_ic=1e4, dt=0.02, tf=1.0, seed=None, testing=False):\n",
        "    \"\"\"\n",
        "    :param tf: final time. default is 15.\n",
        "    :param dt: delta t.\n",
        "    :param x_lower1: lower bound of x1, initial condition.\n",
        "    :param x_upper1: upper bound of x1, initial condition.\n",
        "    :param x_upper2: lower bound of x2, initial condition.\n",
        "    :param x_lower2: upper bound of x1, initial condition.\n",
        "    :param n_side: number of initial conditions on each axis. default is 100.\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    # set seed\n",
        "    np.random.seed(seed=seed)\n",
        "\n",
        "    # dim - time steps\n",
        "    nsteps = int(tf / dt)\n",
        "\n",
        "    # number of initial conditions.\n",
        "    n_ic = int(n_ic)\n",
        "\n",
        "    # create initial condition grid\n",
        "    if testing:\n",
        "        icond1 = np.linspace(x_lower1, x_upper1, 10)\n",
        "        icond2 = np.linspace(x_lower2, x_upper2, 2)\n",
        "        xx, yy = np.meshgrid(icond1, icond2)\n",
        "\n",
        "        # solve the system using Runge–Kutta 4th order method, see rk4 function above.\n",
        "        data_mat = np.zeros((n_ic, 2, nsteps + 1), dtype=np.float64)\n",
        "        ic = 0\n",
        "        for x1 in range(2):\n",
        "            for x2 in range(10):\n",
        "                data_mat[ic, :, 0] = np.array([xx[x1, x2], yy[x1, x2]], dtype=np.float64)\n",
        "                for jj in range(nsteps):\n",
        "                    data_mat[ic, :, jj + 1] = rk4(data_mat[ic, :, jj], dt, dyn_sys_discrete)\n",
        "                ic += 1\n",
        "    else:\n",
        "        icond1 = np.random.uniform(x_lower1, x_upper1, n_ic)\n",
        "        icond2 = np.random.uniform(x_lower2, x_upper2, n_ic)\n",
        "\n",
        "        # solve the system using Runge–Kutta 4th order method, see rk4 function above.\n",
        "        data_mat = np.zeros((n_ic, 2, nsteps + 1), dtype=np.float64)\n",
        "        for ii in range(n_ic):\n",
        "            data_mat[ii, :, 0] = np.array([icond1[ii], icond2[ii]], dtype=np.float64)\n",
        "            for jj in range(nsteps):\n",
        "                data_mat[ii, :, jj + 1] = rk4(data_mat[ii, :, jj], dt, dyn_sys_discrete)\n",
        "\n",
        "    return np.transpose(data_mat, [0, 2, 1])\n",
        "\n",
        "def data_maker_pendulum(x_lower1, x_upper1, x_lower2, x_upper2, n_ic=10000, dt=0.02, tf=1.0, seed=None):\n",
        "    \"\"\"\n",
        "    :param tf: final time. default is 15.\n",
        "    :param dt: delta t.\n",
        "    :param x_lower1: lower bound of x1, initial condition.\n",
        "    :param x_upper1: upper bound of x1, initial condition.\n",
        "    :param x_upper2: lower bound of x2, initial condition.\n",
        "    :param x_lower2: upper bound of x1, initial condition.\n",
        "    :param n_ic: number of initial conditions\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    # set seed\n",
        "    np.random.seed(seed=seed)\n",
        "\n",
        "    # dim - time steps\n",
        "    nsteps = int(tf / dt)\n",
        "\n",
        "    # number of initial conditions\n",
        "    n_ic = int(n_ic)\n",
        "\n",
        "    # create initial condition grid\n",
        "    rand_x1 = np.random.uniform(x_lower1, x_upper1, 100 * n_ic)\n",
        "    rand_x2 = np.random.uniform(x_lower2, x_upper2, 100 * n_ic)\n",
        "    max_potential = 0.99\n",
        "    potential = lambda x, y: (1 / 2) * y ** 2 - np.cos(x)\n",
        "    iconds = np.asarray([[x, y] for x, y in zip(rand_x1, rand_x2)\n",
        "                         if potential(x, y) <= max_potential])[:n_ic, :]\n",
        "\n",
        "    # solve the system using Runge–Kutta 4th order method, see rk4 function above\n",
        "    data_mat = np.zeros((n_ic, 2, nsteps + 1), dtype=np.float64)\n",
        "    for ii in range(n_ic):\n",
        "        data_mat[ii, :, 0] = np.array([iconds[ii, 0], iconds[ii, 1]], dtype=np.float64)\n",
        "        for jj in range(nsteps):\n",
        "            data_mat[ii, :, jj + 1] = rk4(data_mat[ii, :, jj], dt, dyn_sys_pendulum)\n",
        "\n",
        "    return np.transpose(data_mat, [0, 2, 1])\n",
        "\n",
        "def data_maker_pendulum_uniform(n_ic=10000, dt=0.02, tf=3.0, seed=None):\n",
        "    nsteps = np.int(tf / dt)\n",
        "    n_ic = np.int(n_ic)\n",
        "    rand_x1 = np.random.uniform(-3.1, 0, 100*n_ic)\n",
        "    rand_x2 = np.zeros((100*n_ic))\n",
        "    max_potential = 0.99\n",
        "    potential = lambda x, y: (1 / 2) * y ** 2 - np.cos(x)\n",
        "    iconds = np.asarray([[x, y] for x, y in zip(rand_x1, rand_x2)\n",
        "                         if potential(x, y) <= max_potential])[:n_ic, :]\n",
        "    data_mat = np.zeros((n_ic, 2, nsteps + 1), dtype=np.float64)\n",
        "    for ii in range(n_ic):\n",
        "        data_mat[ii, :, 0] = np.array([iconds[ii, 0], iconds[ii, 1]], dtype=np.float64)\n",
        "        for jj in range(nsteps):\n",
        "            data_mat[ii, :, jj + 1] = rk4(data_mat[ii, :, jj], dt, dyn_sys_pendulum)\n",
        "\n",
        "    return np.transpose(data_mat, [0, 2, 1])\n",
        "\n",
        "def data_maker_fluid_flow_slow(r_lower=0, r_upper=1.1, t_lower=0, t_upper=2*np.pi, n_ic=1e4, dt=0.05, tf=6, seed=None):\n",
        "    \"\"\"\n",
        "    :param r_lower: lower bound for r. Default is 0.\n",
        "    :param r_upper: Upper bound for r. Default is 1.\n",
        "    :param t_lower: Lower bound for theta. Default is 0.\n",
        "    :param t_upper: Upper bound for theta. Default is 2pi.\n",
        "    :param n_ic: number of initial conditions. Default is 10000.\n",
        "    :param dt: time step size. Default is 0.05.\n",
        "    :param tf: final time. default is 6.\n",
        "    :return: csv file\n",
        "    \"\"\"\n",
        "    # set seed\n",
        "    np.random.seed(seed=seed)\n",
        "\n",
        "    # dim - time steps\n",
        "    nsteps = int(tf / dt)\n",
        "\n",
        "    # number of initial conditions for slow manifold.\n",
        "    n_ic_slow = int(n_ic)\n",
        "\n",
        "    # create initial condition grid.\n",
        "    r = np.random.uniform(r_lower, r_upper, n_ic_slow)\n",
        "    theta = np.random.uniform(t_lower, t_upper, n_ic_slow)\n",
        "\n",
        "    # compute x1, x2, and x3, based on theta and r\n",
        "    x1 = r * np.cos(theta)\n",
        "    x2 = r * np.sin(theta)\n",
        "    x3 = np.power(x1, 2) + np.power(x2, 2)\n",
        "\n",
        "    # initialize initial conditions matrix.\n",
        "    iconds = np.zeros((n_ic_slow, 3))\n",
        "\n",
        "    # initial conditions for slow manifold.\n",
        "    iconds[:n_ic_slow] = np.asarray([[x, y, z] for x, y, z in zip(x1, x2, x3)])\n",
        "\n",
        "    # solve the system using Runge–Kutta 4th order method, see rk4 function above.\n",
        "    data_mat = np.zeros((n_ic_slow, 3, nsteps + 1), dtype=np.float64)\n",
        "    for ii in range(n_ic_slow):\n",
        "        data_mat[ii, :, 0] = np.array([iconds[ii, 0], iconds[ii, 1], iconds[ii, 2]], dtype=np.float64)\n",
        "        for jj in range(nsteps):\n",
        "            data_mat[ii, :, jj + 1] = rk4(data_mat[ii, :, jj], dt, dyn_sys_fluid)\n",
        "\n",
        "    return np.transpose(data_mat, [0, 2, 1])\n",
        "\n",
        "def data_maker_fluid_flow_full(x1_lower=-1.1, x1_upper=1.1, x2_lower=-1.1, x2_upper=1.1, x3_lower=0.0, x3_upper=2.43,\n",
        "                               n_ic=1e4, dt=0.05, tf=6, seed=None):\n",
        "    # set seed\n",
        "    np.random.seed(seed=seed)\n",
        "\n",
        "    # Number of time steps\n",
        "    nsteps = np.int(tf / dt)\n",
        "\n",
        "    # Number of initial conditions\n",
        "    n_ic = np.int(n_ic)\n",
        "\n",
        "    # Create initial condition grid\n",
        "    x1 = np.random.uniform(x1_lower, x1_upper, n_ic)\n",
        "    x2 = np.random.uniform(x2_lower, x2_upper, n_ic)\n",
        "    x3 = np.random.uniform(x3_lower, x3_upper, n_ic)\n",
        "\n",
        "    # Initialize initial conditions matrix\n",
        "    iconds = np.zeros((n_ic, 3))\n",
        "\n",
        "    # Initial conditions zip\n",
        "    iconds[:n_ic] = np.asarray([[x, y, z] for x, y, z in zip(x1, x2, x3)])\n",
        "\n",
        "    # Solve the system using Runge–Kutta 4th order method, see rk4 function above\n",
        "    data_mat = np.zeros((n_ic, 3, nsteps+1), dtype=np.float64)\n",
        "    for ii in range(n_ic):\n",
        "        data_mat[ii, :, 0] = np.array([iconds[ii, 0], iconds[ii, 1], iconds[ii, 2]], dtype=np.float64)\n",
        "        for jj in range(nsteps):\n",
        "            data_mat[ii, :, jj+1] = rk4(data_mat[ii, :, jj], dt, dyn_sys_fluid)\n",
        "\n",
        "    return np.transpose(data_mat, [0, 2, 1])\n",
        "\n",
        "def data_maker_kdv(x_lower1, x_upper1, x_lower2, x_upper2, n_ic=10000, dt=0.01, tf=1.0, seed=None):\n",
        "    # Setup\n",
        "    np.random.seed(seed=seed)\n",
        "    nsteps = int(tf / dt)\n",
        "    n_ic = int(n_ic)\n",
        "    # Generate initial conditions\n",
        "    icond1 = np.random.uniform(x_lower1, x_upper1, 10*n_ic)\n",
        "    icond2 = np.random.uniform(x_lower2, x_upper2, 10*n_ic)\n",
        "    n_try = 10*n_ic\n",
        "    # Integrate\n",
        "    data_mat = np.zeros((n_try, 2, nsteps+1), dtype=np.float64)\n",
        "    for ii in range(n_try):\n",
        "        data_mat[ii, :, 0] = np.array([icond1[ii], icond2[ii]], dtype=np.float64)\n",
        "        for jj in range(nsteps):\n",
        "            data_mat[ii, :, jj+1] = rk4(data_mat[ii, :, jj], dt, dyn_sys_kdv)\n",
        "            # if (data_mat[ii, 0, jj+1] < x_lower1 or data_mat[ii, 1, jj+1] > x_upper1\n",
        "            #         or data_mat[ii, 1, jj+1] < x_lower2 or data_mat[ii, 1, jj+1] > x_upper2):\n",
        "            #     break\n",
        "    accept = np.abs(data_mat[:, 0, -1]) < 3\n",
        "    data_mat = data_mat[accept, :, :]\n",
        "    accept = np.abs(data_mat[:, 1, -1]) < 3\n",
        "    data_mat = data_mat[accept, :, :]\n",
        "    data_mat = data_mat[:n_ic, :, :]\n",
        "    return np.transpose(data_mat, [0, 2, 1])\n",
        "\n",
        "def data_maker_duffing_driven(x_lower1, x_upper1, x_lower2, x_upper2, n_ic=10000, dt=0.01, tf=1.0, seed=None):\n",
        "    # Setup\n",
        "    np.random.seed(seed=seed)\n",
        "    nsteps = int(tf / dt)\n",
        "    n_ic = int(n_ic)\n",
        "    # Generate initial conditions\n",
        "    icond1 = np.random.uniform(x_lower1, x_upper1, n_ic)\n",
        "    icond2 = np.random.uniform(x_lower2, x_upper2, n_ic)\n",
        "    # Integrate\n",
        "    data_mat = np.zeros((n_ic, 3, nsteps+1), dtype=np.float64)\n",
        "    for ii in range(n_ic):\n",
        "        data_mat[ii, :2, 0] = np.array([icond1[ii], icond2[ii]], dtype=np.float64)\n",
        "        for jj in range(nsteps):\n",
        "            data_mat[ii, :, jj+1] = rk4(data_mat[ii, :, jj], dt, dyn_sys_duffing_driven)\n",
        "            data_mat[ii, 2, jj+1] = data_mat[ii, 2, jj] + dt\n",
        "    return np.transpose(data_mat, [0, 2, 1])\n",
        "\n",
        "def data_maker_duffing(x_lower1, x_upper1, x_lower2, x_upper2, n_ic=10000, dt=0.01, tf=1.0, seed=None):\n",
        "    # Setup\n",
        "    np.random.seed(seed=seed)\n",
        "    nsteps = int(tf / dt)\n",
        "    n_ic = int(n_ic)\n",
        "    # Generate initial conditions\n",
        "    icond1 = np.random.uniform(x_lower1, x_upper1, n_ic)\n",
        "    icond2 = np.random.uniform(x_lower2, x_upper2, n_ic)\n",
        "    # Integrate\n",
        "    data_mat = np.zeros((n_ic, 2, nsteps+1), dtype=np.float64)\n",
        "    for ii in range(n_ic):\n",
        "        data_mat[ii, :, 0] = np.array([icond1[ii], icond2[ii]], dtype=np.float64)\n",
        "        for jj in range(nsteps):\n",
        "            data_mat[ii, :, jj+1] = rk4(data_mat[ii, :, jj], dt, dyn_sys_duffing)\n",
        "    return np.transpose(data_mat, [0, 2, 1])\n",
        "\n",
        "def data_maker_duffing_bollt(x_lower1, x_upper1, x_lower2, x_upper2, n_ic=10000, dt=0.01, tf=1.0, seed=None):\n",
        "    # Setup\n",
        "    np.random.seed(seed=seed)\n",
        "    nsteps = int(tf / dt)\n",
        "    n_ic = int(n_ic)\n",
        "    # Generate initial conditions\n",
        "    icond1 = np.random.uniform(x_lower1, x_upper1, n_ic)\n",
        "    icond2 = np.random.uniform(x_lower2, x_upper2, n_ic)\n",
        "    # Integrate\n",
        "    data_mat = np.zeros((n_ic, 2, nsteps+1), dtype=np.float64)\n",
        "    for ii in range(n_ic):\n",
        "        data_mat[ii, :, 0] = np.array([icond1[ii], icond2[ii]], dtype=np.float64)\n",
        "        for jj in range(nsteps):\n",
        "            data_mat[ii, :, jj+1] = rk4(data_mat[ii, :, jj], dt, dyn_sys_duffing_bollt)\n",
        "    return np.transpose(data_mat, [0, 2, 1])\n",
        "\n",
        "# ==============================================================================\n",
        "# Test program\n",
        "# ==============================================================================\n",
        "if __name__ == \"__main__\":\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    create_discrete = False\n",
        "    create_pendulum = False\n",
        "    create_fluid_flow_slow = False\n",
        "    create_fluid_flow_full = False\n",
        "    create_kdv = False\n",
        "    create_duffing = True\n",
        "\n",
        "    if create_discrete:\n",
        "        # Generate the data\n",
        "        data = data_maker_discrete(x_lower1=-0.5, x_upper1=0.5, x_lower2=-0.5, x_upper2=0.5, n_ic=20, dt=0.02, tf=10)\n",
        "        # Visualize\n",
        "        plt.figure(1, figsize=(8, 8))\n",
        "        for ii in range(data.shape[0]):\n",
        "            plt.plot(data[ii, :, 0], data[ii, :, 1], '-')\n",
        "        plt.xlabel(\"x1\", fontsize=18)\n",
        "        plt.ylabel(\"X2\", fontsize=18)\n",
        "        plt.title(\"Discrete dataset\", fontsize=18)\n",
        "\n",
        "    if create_pendulum:\n",
        "        # Generate the data\n",
        "        data = data_maker_pendulum(x_lower1=-3.1, x_upper1=3.1, x_lower2=-2, x_upper2=2, n_ic=20, dt=0.02, tf=20)\n",
        "        # Visualize\n",
        "        plt.figure(2, figsize=(8, 8))\n",
        "        for ii in range(data.shape[0]):\n",
        "            plt.plot(data[ii, :, 0], data[ii, :, 1], '-')\n",
        "        plt.xlabel(\"x1\", fontsize=18)\n",
        "        plt.ylabel(\"X2\", fontsize=18)\n",
        "        plt.title(\"Pendulum dataset\", fontsize=18)\n",
        "\n",
        "    if create_fluid_flow_slow:\n",
        "        # Generate the data\n",
        "        data = data_maker_fluid_flow_slow(r_lower=0, r_upper=1.1, t_lower=0, t_upper=2*np.pi, n_ic=20, dt=0.05, tf=10)\n",
        "        # Visualize\n",
        "        fig = plt.figure(3, figsize=(8, 8))\n",
        "        ax = plt.axes(projection='3d')\n",
        "        for ii in range(data.shape[0]):\n",
        "            ax.plot3D(data[ii, :, 0], data[ii, :, 1], data[ii, :, 2])\n",
        "        ax.set_xlabel(\"$x_{1}$\", fontsize=18)\n",
        "        ax.set_ylabel(\"$x_{2}$\", fontsize=18)\n",
        "        ax.set_zlabel(\"$x_{3}$\", fontsize=18)\n",
        "        plt.title(\"Fluid Flow dataset\", fontsize=20)\n",
        "\n",
        "    if create_fluid_flow_full:\n",
        "        # Generate the data\n",
        "        data = data_maker_fluid_flow_full(x1_lower=-1.1, x1_upper=1.1, x2_lower=-1.1, x2_upper=1.1,\n",
        "                                          x3_lower=0.0, x3_upper=2.43, n_ic=20, dt=0.05, tf=6)\n",
        "        # Visualize\n",
        "        fig = plt.figure(4, figsize=(8, 8))\n",
        "        ax = plt.axes(projection='3d')\n",
        "        for ii in range(data.shape[0]):\n",
        "            ax.plot3D(data[ii, :, 0], data[ii, :, 1], data[ii, :, 2])\n",
        "        ax.set_xlabel(\"$x_{1}$\", fontsize=18)\n",
        "        ax.set_ylabel(\"$x_{2}$\", fontsize=18)\n",
        "        ax.set_zlabel(\"$x_{3}$\", fontsize=18)\n",
        "        plt.title(\"Fluid Flow dataset\", fontsize=20)\n",
        "\n",
        "    if create_kdv:\n",
        "        # Generate the data\n",
        "        data = data_maker_kdv(x_lower1=-2, x_upper1=2, x_lower2=-2, x_upper2=2, n_ic=1000, dt=0.01, tf=20)\n",
        "        # Visualize\n",
        "        plt.figure(2, figsize=(8, 8))\n",
        "        for ii in range(data.shape[0]):\n",
        "            npts = np.sum(np.abs(data[ii, :, 0]) > 0)\n",
        "            plt.plot(data[ii, :npts, 0], data[ii, :npts, 1], 'r-', lw=0.25)\n",
        "        plt.xlabel(\"x1\", fontsize=18)\n",
        "        plt.ylabel(\"X2\", fontsize=18)\n",
        "        plt.title(\"KdV dataset\", fontsize=18)\n",
        "\n",
        "    if create_duffing:\n",
        "        # Generate the data\n",
        "        data = data_maker_duffing(x_lower1=-1, x_upper1=1, x_lower2=-1, x_upper2=1, n_ic=2, dt=0.05, tf=200)\n",
        "        # Visualize\n",
        "        plt.figure(2, figsize=(8, 8))\n",
        "        plt.plot(data[0, :, 0], data[0, :, 1], 'r-', lw=0.5)\n",
        "        plt.plot(data[1, :, 0], data[1, :, 1], 'b-', lw=0.5)\n",
        "        plt.xlabel(\"x1\", fontsize=18)\n",
        "        plt.ylabel(\"X2\", fontsize=18)\n",
        "        plt.title(\"Duffing oscillator\", fontsize=18)\n",
        "\n",
        "    plt.show()\n",
        "    print(\"done\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "    Created by:\n",
        "        Jay Lago\n",
        "\n",
        "    Modified:\n",
        "        17 Nov 2020 - Jay Lago\n",
        "\"\"\"\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import matplotlib\n",
        "import numpy as np\n",
        "\n",
        "font = {'family': 'DejaVu Sans', 'size': 18}\n",
        "matplotlib.rc('font', **font)\n",
        "\n",
        "\n",
        "def diagnostic_plot(y_pred, y_true, hyp_params, epoch, save_path, loss_comps, val_loss):\n",
        "    if hyp_params['experiment'] == 'discrete' or \\\n",
        "            hyp_params['experiment'] == 'pendulum' or \\\n",
        "            hyp_params['experiment'] == 'van_der_pol' or \\\n",
        "            hyp_params['experiment'] == 'duffing' or \\\n",
        "            hyp_params['experiment'] == 'kdv':\n",
        "        plot_2D(y_pred, y_true, hyp_params, epoch, save_path, loss_comps, val_loss)\n",
        "    elif hyp_params['experiment'] == 'fluid_flow_slow' or \\\n",
        "            hyp_params['experiment'] == 'fluid_flow_full':\n",
        "        plot_3d(y_pred, y_true, hyp_params, epoch, save_path, loss_comps, val_loss)\n",
        "    else:\n",
        "        print(\"unknown experiment, create new diagnostic plots\")\n",
        "\n",
        "def net_steps_plot(num_steps):\n",
        "    \n",
        "    plt.plot(num_steps)\n",
        "    plt.title(\"Delay Path\")\n",
        "    plt.ylabel(\"Tau\")\n",
        "    plt.xlabel(\"batch #\")\n",
        "    plt.savefig(\"num_steps.png\")\n",
        "    plt.close()\n",
        "        \n",
        "def loss_diff_plot(loss_diff):\n",
        "    plt.plot(np.abs(loss_diff))\n",
        "    plt.title(\"Difference in step loss\")\n",
        "    plt.ylabel(\"Loss Difference\")\n",
        "    plt.yscale(\"log\")\n",
        "    plt.savefig(\"val_loss_dif.png\")\n",
        "    plt.close()\n",
        "        \n",
        "        \n",
        "def plot_2D(y_pred, y_true, hyp_params, epoch, save_path, loss_comps, val_loss):\n",
        "    enc = y_pred[0]\n",
        "    enc_dec = y_pred[1]\n",
        "    enc_adv_dec = y_pred[2]\n",
        "    enc_adv = y_pred[3]\n",
        "\n",
        "    fig, ax = plt.subplots(nrows=3, ncols=3, sharex=False, sharey=False, figsize=(40, 20))\n",
        "    ax = ax.flat\n",
        "    skip = 1\n",
        "\n",
        "    # Validation batch\n",
        "    for ii in np.arange(0, y_true.shape[0], skip):\n",
        "        ax[0].plot(y_true[ii, :, 0], y_true[ii, :, 1], '-')\n",
        "    ax[0].scatter(y_true[:, 0, 0], y_true[:, 0, 1])\n",
        "    ax[0].grid()\n",
        "    ax[0].set_xlabel(\"x1\")\n",
        "    ax[0].set_ylabel(\"x2\")\n",
        "    ax[0].set_title(\"Validation Data (x)\")\n",
        "\n",
        "    # Encoded-advanced-decoded time series\n",
        "    for ii in np.arange(0, enc_adv_dec.shape[0], skip):\n",
        "        ax[1].plot(enc_adv_dec[ii, :, 0], enc_adv_dec[ii, :, 1], '-')\n",
        "    ax[1].scatter(enc_adv_dec[:, 0, 0], enc_adv_dec[:, 0, 1])\n",
        "    ax[1].grid()\n",
        "    ax[1].set_xlabel(\"x1\")\n",
        "    ax[1].set_ylabel(\"x2\")\n",
        "    ax[1].set_title(\"Encoded-Advanced-Decoded (x_adv))\")\n",
        "\n",
        "    # Encoded time series\n",
        "    for ii in np.arange(0, enc.shape[0], skip):\n",
        "        ax[2].plot(enc[ii, :, 0], enc[ii, :, 1], '-')\n",
        "    ax[2].scatter(enc[:, 0, 0], enc[:, 0, 1])\n",
        "    ax[2].grid()\n",
        "    ax[2].set_xlabel(\"y1\")\n",
        "    ax[2].set_ylabel(\"y2\")\n",
        "    ax[2].axis(\"equal\")\n",
        "    ax[2].set_title(\"Encoded (y)\")\n",
        "\n",
        "    # Encoded-decoded time series\n",
        "    for ii in np.arange(0, enc_dec.shape[0], skip):\n",
        "        ax[3].plot(enc_dec[ii, :, 0], enc_dec[ii, :, 1], '-')\n",
        "    ax[3].scatter(enc_dec[:, 0, 0], enc_dec[:, 0, 1])\n",
        "    ax[3].grid()\n",
        "    ax[3].set_xlabel(\"x1\")\n",
        "    ax[3].set_ylabel(\"x2\")\n",
        "    ax[3].set_title(\"Encoded-Decoded (x_ae)\")\n",
        "\n",
        "    # Encoded-advanced time series\n",
        "    for ii in np.arange(0, enc_adv.shape[0], skip):\n",
        "        ax[4].plot(enc_adv[ii, :, 0], enc_adv[ii, :, 1], '-')\n",
        "    ax[4].scatter(enc_adv[:, 0, 0], enc_adv[:, 0, 1])\n",
        "    ax[4].grid()\n",
        "    ax[4].set_xlabel(\"y1\")\n",
        "    ax[4].set_ylabel(\"y2\")\n",
        "    ax[4].axis(\"equal\")\n",
        "    ax[4].set_title(\"Encoded-Advanced (y_adv))\")\n",
        "\n",
        "    # Loss components\n",
        "    lw = 3\n",
        "    loss_comps = np.asarray(loss_comps)\n",
        "    ax[5].plot(val_loss, color='k', linewidth=lw, label='total')\n",
        "    ax[5].set_title(\"Total Loss\")\n",
        "    ax[5].grid()\n",
        "    ax[5].set_xlabel(\"Epoch\")\n",
        "    ax[5].set_ylabel(\"$log_{10}(L)$\")\n",
        "    ax[5].legend(loc=\"upper right\")\n",
        "\n",
        "    ax[6].plot(loss_comps[:, 0], color='r', linewidth=lw, label='recon')\n",
        "    ax[6].set_title(\"Recon Loss\")\n",
        "    ax[6].grid()\n",
        "    ax[6].set_xlabel(\"Epoch\")\n",
        "    ax[6].set_ylabel(\"$log_{10}(L_{recon})$\")\n",
        "    ax[6].legend(loc=\"upper right\")\n",
        "\n",
        "    ax[7].plot(loss_comps[:, 1], color='b', linewidth=lw, label='pred')\n",
        "    ax[7].set_title(\"Prediction Loss\")\n",
        "    ax[7].grid()\n",
        "    ax[7].set_xlabel(\"Epoch\")\n",
        "    ax[7].set_ylabel(\"$log_{10}(L_{pred})$\")\n",
        "    ax[7].legend(loc=\"upper right\")\n",
        "\n",
        "    ax[8].plot(loss_comps[:, 2], color='g', linewidth=lw, label='dmd')\n",
        "    ax[8].set_title(\"DMD\")\n",
        "    ax[8].grid()\n",
        "    ax[8].set_xlabel(\"Epoch\")\n",
        "    ax[8].set_ylabel(\"$log_{10}(L_{dmd})$\")\n",
        "    ax[8].legend(loc=\"upper right\")\n",
        "\n",
        "    fig.suptitle(\n",
        "        \"Epoch: {cur_epoch}/{max_epoch}, Learn Rate: {lr:.5f}, Val. Loss: {loss:.3f}\".format(\n",
        "            cur_epoch=epoch,\n",
        "            max_epoch=hyp_params['max_epochs'],\n",
        "            lr=hyp_params['lr'],\n",
        "            loss=val_loss[-1]))\n",
        "\n",
        "    plt.savefig(save_path)\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def plot_3d(y_pred, y_true, hyp_params, epoch, save_path, loss_comps, val_loss):\n",
        "    enc = y_pred[0]\n",
        "    enc_dec = y_pred[1]\n",
        "    enc_adv_dec = y_pred[2]\n",
        "    enc_adv = y_pred[3]\n",
        "\n",
        "    font = {'family': 'DejaVu Sans', 'size': 10}\n",
        "    matplotlib.rc('font', **font)\n",
        "\n",
        "    skip = 8\n",
        "    fig = plt.figure(figsize=(40, 20))\n",
        "\n",
        "    # Validation batch\n",
        "    ax = fig.add_subplot(3, 3, 1, projection='3d')\n",
        "    for ii in np.arange(0, y_true.shape[0], skip):\n",
        "        ii = int(ii)\n",
        "        x1 = y_true[ii, :, 0]\n",
        "        x2 = y_true[ii, :, 1]\n",
        "        x3 = y_true[ii, :, 2]\n",
        "        ax.plot3D(x1, x2, x3)\n",
        "    ax.set_xlabel(\"$x_{1}$\")\n",
        "    ax.set_ylabel(\"$x_{2}$\")\n",
        "    ax.set_zlabel(\"$x_{3}$\")\n",
        "    ax.set_title(\"Validation Data (x)\")\n",
        "\n",
        "    # Encoded-advanced-decoded time series\n",
        "    ax = fig.add_subplot(3, 3, 2, projection='3d')\n",
        "    for ii in np.arange(0, enc_adv_dec.shape[0], skip):\n",
        "        ii = int(ii)\n",
        "        x1 = enc_adv_dec[ii, :, 0]\n",
        "        x2 = enc_adv_dec[ii, :, 1]\n",
        "        x3 = enc_adv_dec[ii, :, 2]\n",
        "        ax.plot3D(x1, x2, x3)\n",
        "    ax.set_xlabel(\"$x_{1}$\")\n",
        "    ax.set_ylabel(\"$x_{2}$\")\n",
        "    ax.set_zlabel(\"$x_{3}$\")\n",
        "    ax.set_title(\"Encoded-Advanced-Decoded (x_adv))\")\n",
        "\n",
        "    # Encoded time series\n",
        "    ax = fig.add_subplot(3, 3, 3, projection='3d')\n",
        "    for ii in np.arange(0, enc.shape[0], skip):\n",
        "        ii = int(ii)\n",
        "        x1 = enc[ii, :, 0]\n",
        "        x2 = enc[ii, :, 1]\n",
        "        x3 = enc[ii, :, 2]\n",
        "        ax.plot3D(x1, x2, x3)\n",
        "    ax.set_xlabel(\"$y_{1}$\")\n",
        "    ax.set_ylabel(\"$y_{2}$\")\n",
        "    ax.set_zlabel(\"$y_{3}$\")\n",
        "    ax.set_title(\"Encoded (y)\")\n",
        "\n",
        "    # Encoded-decoded time series\n",
        "    ax = fig.add_subplot(3, 3, 4, projection='3d')\n",
        "    for ii in np.arange(0, enc_dec.shape[0], skip):\n",
        "        ii = int(ii)\n",
        "        x1 = enc_dec[ii, :, 0]\n",
        "        x2 = enc_dec[ii, :, 1]\n",
        "        x3 = enc_dec[ii, :, 2]\n",
        "        ax.plot3D(x1, x2, x3)\n",
        "    ax.set_xlabel(\"$x_{1}$\")\n",
        "    ax.set_ylabel(\"$x_{2}$\")\n",
        "    ax.set_zlabel(\"$x_{3}$\")\n",
        "    ax.set_title(\"Encoded-Decoded (x_ae)\")\n",
        "\n",
        "    # Encoded-advanced time series\n",
        "    ax = fig.add_subplot(3, 3, 5, projection='3d')\n",
        "    for ii in np.arange(0, enc_adv.shape[0], skip):\n",
        "        ii = int(ii)\n",
        "        x1 = enc_adv[ii, :, 0]\n",
        "        x2 = enc_adv[ii, :, 1]\n",
        "        x3 = enc_adv[ii, :, 2]\n",
        "        ax.plot3D(x1, x2, x3)\n",
        "    ax.set_xlabel(\"$y_{1}$\")\n",
        "    ax.set_ylabel(\"$y_{2}$\")\n",
        "    ax.set_zlabel(\"$y_{3}$\")\n",
        "    ax.set_title(\"Encoded-Advanced (y_adv))\")\n",
        "\n",
        "    # Loss components\n",
        "    lw = 3\n",
        "    loss_comps = np.asarray(loss_comps)\n",
        "    ax = fig.add_subplot(3, 3, 6)\n",
        "    ax.plot(val_loss, color='k', linewidth=lw, label='total')\n",
        "    ax.set_title(\"Total Loss\")\n",
        "    ax.grid()\n",
        "    ax.set_xlabel(\"Epoch\")\n",
        "    ax.set_ylabel(\"$log_{10}(L)$\")\n",
        "    ax.legend(loc=\"upper right\")\n",
        "\n",
        "    ax = fig.add_subplot(3, 3, 7)\n",
        "    ax.plot(loss_comps[:, 0], color='r', linewidth=lw, label='recon')\n",
        "    ax.set_title(\"Recon Loss\")\n",
        "    ax.grid()\n",
        "    ax.set_xlabel(\"Epoch\")\n",
        "    ax.set_ylabel(\"$log_{10}(L_{recon})$\")\n",
        "    ax.legend(loc=\"upper right\")\n",
        "\n",
        "    ax = fig.add_subplot(3, 3, 8)\n",
        "    ax.plot(loss_comps[:, 1], color='b', linewidth=lw, label='pred')\n",
        "    ax.set_title(\"Prediction Loss\")\n",
        "    ax.grid()\n",
        "    ax.set_xlabel(\"Epoch\")\n",
        "    ax.set_ylabel(\"$log_{10}(L_{pred})$\")\n",
        "    ax.legend(loc=\"upper right\")\n",
        "\n",
        "    ax = fig.add_subplot(3, 3, 9)\n",
        "    ax.plot(loss_comps[:, 2], color='g', linewidth=lw, label='dmd')\n",
        "    ax.set_title(\"DMD\")\n",
        "    ax.grid()\n",
        "    ax.set_xlabel(\"Epoch\")\n",
        "    ax.set_ylabel(\"$log_{10}(L_{dmd})$\")\n",
        "    ax.legend(loc=\"upper right\")\n",
        "\n",
        "    fig.suptitle(\n",
        "        \"Epoch: {cur_epoch}/{max_epoch}, Learn Rate: {lr:.5f}, Val. Loss: {loss:.3f}\".format(\n",
        "            cur_epoch=epoch,\n",
        "            max_epoch=hyp_params['max_epochs'],\n",
        "            lr=hyp_params['lr'],\n",
        "            loss=val_loss[-1]))\n",
        "\n",
        "    plt.savefig(save_path)\n",
        "    plt.close()\n"
      ],
      "metadata": {
        "id": "3bfUhItsW4i5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "    Created by:\n",
        "        Jay Lago - 23 Dec 2020\n",
        "\"\"\"\n",
        "import os\n",
        "import pickle\n",
        "import time\n",
        "import datetime as dt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "def train_model(hyp_params, train_data, val_set, model, loss):\n",
        "    # Dictionary to store all relevant training parameters and losses\n",
        "    train_params = dict()\n",
        "    train_params['start_time'] = time.time()\n",
        "    train_params['train_loss_results'] = []\n",
        "    train_params['val_loss_results'] = []\n",
        "    train_params['val_loss_comps_avgs'] = []\n",
        "    # Step = 1\n",
        "    # total = -16\n",
        "    # total_steps = [total]\n",
        "    # original_window = model.window\n",
        "    for epoch in range(1, hyp_params['max_epochs'] + 1):\n",
        "        epoch_start_time = dt.datetime.now()\n",
        "        epoch_time = time.time()\n",
        "        epoch_loss_avg_train = tf.keras.metrics.Mean()\n",
        "        epoch_loss_avg_val = tf.keras.metrics.Mean()\n",
        "        # epoch_steps = []\n",
        "        # model.window = original_window\n",
        "        # Shuffle, batch, and prefetch training data to the GPU\n",
        "        train_set = train_data.shuffle(hyp_params['num_train_init_conds']) \\\n",
        "            .batch(hyp_params['batch_size'], drop_remainder=True)\n",
        "        train_set = train_set.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "        # Set optimizer\n",
        "        if hyp_params['optimizer'] == 'adam':\n",
        "            myoptimizer = tf.keras.optimizers.Adam(hyp_params['lr'])\n",
        "        if hyp_params['optimizer'] == 'sgd':\n",
        "            myoptimizer = tf.keras.optimizers.SGD(learning_rate=hyp_params['lr'], momentum=0.9)\n",
        "\n",
        "        # Begin batch training\n",
        "        with tf.device(hyp_params['device']):\n",
        "            for train_batch in train_set:\n",
        "                with tf.GradientTape() as tape:\n",
        "                    train_pred = model(train_batch, training=True)\n",
        "                    train_loss = loss(train_pred, train_batch)\n",
        "                gradients = tape.gradient(train_loss, model.trainable_weights)\n",
        "                myoptimizer.apply_gradients([(grad, var) for (grad, var) in zip(gradients, model.trainable_weights)\n",
        "                                             if grad is not None])\n",
        "                myoptimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
        "                epoch_loss_avg_train.update_state(train_loss)\n",
        "\n",
        "            # Batch validation\n",
        "            lrecon = tf.keras.metrics.Mean()\n",
        "            lpred = tf.keras.metrics.Mean()\n",
        "            ldmd = tf.keras.metrics.Mean()\n",
        "\n",
        "            for val_batch in val_set:\n",
        "                val_pred = model(val_batch)\n",
        "                val_loss = loss(val_pred, val_batch)\n",
        "                epoch_loss_avg_val.update_state(val_loss)\n",
        "                # Save loss components for diagnostic plotting\n",
        "                lrecon.update_state(np.log10(loss.loss_recon))\n",
        "                lpred.update_state(np.log10(loss.loss_pred))\n",
        "                ldmd.update_state(np.log10(loss.loss_dmd))\n",
        "                \n",
        "                # if model.window + Step >= model.num_pred_steps:\n",
        "                #     range_list = np.array([-Step, 0])\n",
        "                # elif model.window - Step <= model.num_pred_steps - 80:\n",
        "                #     range_list = np.array([0, Step])\n",
        "                # else:\n",
        "                #     range_list = np.array([-Step, 0, Step])\n",
        "\n",
        "                # val_preds = [None]*np.size(range_list)\n",
        "                # val_losses = np.zeros(np.size(range_list), dtype=np.float64)\n",
        "                # for jj in range(np.size(range_list)):\n",
        "                #     model.window = original_window + range_list[jj]\n",
        "                #     val_preds[jj] = model(val_batch)\n",
        "                #     val_losses[jj] = loss(val_preds[jj], val_batch)\n",
        "\n",
        "                # min_ind = np.argmin(val_losses)\n",
        "                # val_pred = val_preds[min_ind]\n",
        "                # val_loss = val_losses[min_ind]\n",
        "                # epoch_steps.append(range_list[min_ind])               \n",
        "\n",
        "                # epoch_loss_avg_val.update_state(val_loss)\n",
        "                # # Save loss components for diagnostic plotting\n",
        "                # lrecon.update_state(np.log10(loss.loss_recon))\n",
        "                # lpred.update_state(np.log10(loss.loss_pred))\n",
        "                # ldmd.update_state(np.log10(loss.loss_dmd))\n",
        "\n",
        "            train_params['val_loss_comps_avgs'].append([lrecon.result(), lpred.result(), ldmd.result()])\n",
        "\n",
        "        # Report training status\n",
        "        train_params['train_loss_results'].append(np.log10(epoch_loss_avg_train.result()))\n",
        "        train_params['val_loss_results'].append(np.log10(epoch_loss_avg_val.result()))\n",
        "        print(\"Epoch {epoch} of {max_epoch} / Train {train:3.7f} / Val {test:3.7f} / LR {lr:2.7f} / {time:4.2f} seconds\"\n",
        "              .format(epoch=epoch, max_epoch=hyp_params['max_epochs'],\n",
        "                      train=train_params['train_loss_results'][-1],\n",
        "                      test=train_params['val_loss_results'][-1],\n",
        "                      lr=hyp_params['lr'],\n",
        "                      time=time.time() - epoch_time))\n",
        "\n",
        "        # mode = max(set(epoch_steps), key=epoch_steps.count)\n",
        "        # print(\"The most common window shift was: \", mode)\n",
        "        # original_window = original_window + mode\n",
        "        # print(\"New window size for the next epoch: \", original_window)\n",
        "        # total = total + mode\n",
        "        # total_steps.append(total)        \n",
        "        \n",
        "        # Save training diagnostic plots\n",
        "        if epoch == 1 or epoch % hyp_params['plot_every'] == 0:\n",
        "            if not os.path.exists(hyp_params['plot_path']):\n",
        "                os.makedirs(hyp_params['plot_path'])\n",
        "            this_plot = hyp_params['plot_path'] + '/' + epoch_start_time.strftime(\"%Y%m%d%H%M%S\") + '.png'\n",
        "            diagnostic_plot(val_pred, val_batch, hyp_params, epoch,\n",
        "                               this_plot, train_params['val_loss_comps_avgs'],\n",
        "                               train_params['val_loss_results'])\n",
        "\n",
        "        # Save model\n",
        "        if epoch % hyp_params['save_every'] == 0 or epoch == hyp_params['max_epochs']:\n",
        "            if not os.path.exists(hyp_params['model_path']):\n",
        "                os.makedirs(hyp_params['model_path'])\n",
        "            model_path = hyp_params['model_path'] + '/epoch_{epoch}_loss_{loss:2.3}' \\\n",
        "                .format(epoch=epoch, loss=train_params['val_loss_results'][-1])\n",
        "            model.save_weights(model_path + '.h5')\n",
        "            pickle.dump(hyp_params, open(model_path + '.pkl', 'wb'))\n",
        "\n",
        "    print(\"\\nTotal training time: %4.2f minutes\" % ((time.time() - train_params['start_time']) / 60.0))\n",
        "    print(\"Final train loss: %2.7f\" % (train_params['train_loss_results'][-1]))\n",
        "    print(\"Final validation loss: %2.7f\" % (train_params['val_loss_results'][-1]))\n",
        "    # print(\"here are the delay steps taken: \")\n",
        "    # print(total_steps)\n",
        "    # hf.net_steps_plot(total_steps)\n",
        "    results = dict()\n",
        "    results['model'] = model\n",
        "    results['loss'] = loss\n",
        "    results['val_loss_history'] = train_params['val_loss_results']\n",
        "    results['val_loss_comps'] = train_params['val_loss_comps_avgs']\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "rrC_jeoXW-jI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "    Author:\n",
        "        Jay Lago, NIWC/SDSU, 2021\n",
        "\"\"\"\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import *\n",
        "\n",
        "\n",
        "class HDMD(keras.Model):\n",
        "    def __init__(self, hyp_params, **kwargs):\n",
        "        super(HDMD, self).__init__(**kwargs)\n",
        "\n",
        "        # Parameters\n",
        "        self.batch_size = hyp_params['batch_size']\n",
        "        self.phys_dim = hyp_params['phys_dim']\n",
        "        self.latent_dim = hyp_params['latent_dim']\n",
        "        self.num_time_steps = int(hyp_params['num_time_steps'])\n",
        "        self.num_pred_steps = int(hyp_params['num_pred_steps'])\n",
        "        self.time_final = hyp_params['time_final']\n",
        "        self.num_en_layers = hyp_params['num_en_layers']\n",
        "        self.num_neurons = hyp_params['num_en_neurons']\n",
        "        self.delta_t = hyp_params['delta_t']\n",
        "        self.precision = hyp_params['precision']\n",
        "\n",
        "        self.num_observables = hyp_params['num_observables']\n",
        "        self.threshold = hyp_params['threshold']\n",
        "        self.observation_dimension = hyp_params['observation_dimension']\n",
        "        self.window = self.num_time_steps - (self.num_observables - 1)\n",
        "\n",
        "        self.enc_input = (self.num_time_steps, self.phys_dim)\n",
        "        self.dec_input = (self.window-1, self.latent_dim)\n",
        "\n",
        "        if self.precision == 'float32':\n",
        "            self.precision_complex = tf.complex64\n",
        "        else:\n",
        "            self.precision_complex = tf.complex128\n",
        "\n",
        "        # Construct the ENCODER network\n",
        "        self.encoder = keras.Sequential(name=\"encoder\")\n",
        "        self.encoder.add(Dense(self.num_neurons,\n",
        "                               input_shape=self.enc_input,\n",
        "                               activation=hyp_params['hidden_activation'],\n",
        "                               kernel_initializer=hyp_params['kernel_init_enc'],\n",
        "                               bias_initializer=hyp_params['bias_initializer'],\n",
        "                               trainable=True, name='enc_in'))\n",
        "        for ii in range(self.num_en_layers):\n",
        "            self.encoder.add(Dense(self.num_neurons,\n",
        "                                   activation=hyp_params['hidden_activation'],\n",
        "                                   kernel_initializer=hyp_params['kernel_init_enc'],\n",
        "                                   bias_initializer=hyp_params['bias_initializer'],\n",
        "                                   trainable=True, name='enc_' + str(ii)))\n",
        "        self.encoder.add(Dense(self.latent_dim,\n",
        "                               activation=hyp_params['ae_output_activation'],\n",
        "                               kernel_initializer=hyp_params['kernel_init_enc'],\n",
        "                               bias_initializer=hyp_params['bias_initializer'],\n",
        "                               trainable=True, name='enc_out'))\n",
        "\n",
        "        # Construct the DECODER network\n",
        "        self.decoder = keras.Sequential(name=\"decoder\")\n",
        "        self.decoder.add(Dense(self.num_neurons,\n",
        "                               input_shape=self.dec_input,\n",
        "                               activation=hyp_params['hidden_activation'],\n",
        "                               kernel_initializer=hyp_params['kernel_init_enc'],\n",
        "                               bias_initializer=hyp_params['bias_initializer'],\n",
        "                               trainable=True, name='dec_in'))\n",
        "        for ii in range(self.num_en_layers):\n",
        "            self.decoder.add(Dense(self.num_neurons,\n",
        "                                   activation=hyp_params['hidden_activation'],\n",
        "                                   kernel_initializer=hyp_params['kernel_init_dec'],\n",
        "                                   bias_initializer=hyp_params['bias_initializer'],\n",
        "                                   trainable=True, name='dec_' + str(ii)))\n",
        "        self.decoder.add(Dense(self.phys_dim,\n",
        "                               activation=hyp_params['ae_output_activation'],\n",
        "                               kernel_initializer=hyp_params['kernel_init_dec'],\n",
        "                               bias_initializer=hyp_params['bias_initializer'],\n",
        "                               trainable=True, name='dec_out'))\n",
        "\n",
        "    def call(self, x):\n",
        "        # Encode the entire time series\n",
        "        y = self.encoder(x)\n",
        "        x_ae = self.decoder(y[:, :(self.window-1), :])\n",
        "\n",
        "        # Reshape for DMD step\n",
        "        yt = tf.transpose(y, [0, 2, 1])\n",
        "\n",
        "        y_adv, evals, evecs, phi = self.hankel_dmd(yt)\n",
        "\n",
        "        # Generate latent time series using DMD prediction\n",
        "        #y_adv, evals, evecs, phi = self.edmd(yt)\n",
        "\n",
        "        # Decode the latent trajectories\n",
        "        x_adv = self.decoder(y_adv)\n",
        "\n",
        "        # Model weights\n",
        "        weights = self.trainable_weights\n",
        "\n",
        "        return [y, x_ae, x_adv, y_adv, weights, evals, evecs, phi]\n",
        "\n",
        "    def edmd(self, Y):\n",
        "        Y_m = Y[:, :, :-1]\n",
        "        Y_p = Y[:, :, 1:]\n",
        "\n",
        "        sig, U, V = tf.linalg.svd(Y_m, compute_uv=True, full_matrices=False)\n",
        "        sigr_inv = tf.linalg.diag(1.0 / sig)\n",
        "        Uh = tf.linalg.adjoint(U)\n",
        "\n",
        "        A = Y_p @ V @ sigr_inv @ Uh\n",
        "        evals, evecs = tf.linalg.eig(A)\n",
        "        phi = tf.linalg.solve(evecs, tf.cast(Y_m, dtype=self.precision_complex))\n",
        "        y0 = phi[:, :, 0]\n",
        "        y0 = y0[:, :, tf.newaxis]\n",
        "\n",
        "        recon = tf.TensorArray(self.precision_complex, size=self.num_pred_steps)\n",
        "        recon = recon.write(0, evecs @ y0)\n",
        "        evals_k = tf.identity(evals)\n",
        "        for ii in tf.range(1, self.num_pred_steps):\n",
        "            tmp = evecs @ (tf.linalg.diag(evals_k) @ y0)\n",
        "            recon = recon.write(ii, tmp)\n",
        "            evals_k = evals_k * evals\n",
        "        recon = tf.math.real(tf.transpose(tf.squeeze(recon.stack()), perm=[1, 0, 2]))\n",
        "        return recon, evals, evecs, phi\n",
        "\n",
        "    def hankel_matrix(self, tseries):\n",
        "        # note, we are working exclusively in NumPy/SciPy here\n",
        "        tcol = tseries[:self.num_observables]\n",
        "        trow = tseries[(self.num_observables-1):]\n",
        "        hmat = np.flipud(sp.linalg.toeplitz(tcol[::-1], trow))\n",
        "        return hmat\n",
        "\n",
        "    def hankel_dmd(self, Y):\n",
        "        winsize = self.window\n",
        "        nobs = self.num_observables\n",
        "        # Perform DMD method.  Note, we need to be careful about how we break the concantenated Hankel matrix apart.\n",
        "\n",
        "        gm = tf.Variable(tf.zeros([self.num_observables, self.batch_size * (self.window - 1)], dtype=self.precision))\n",
        "        gp = tf.Variable(tf.zeros([self.num_observables, self.batch_size * (self.window - 1)], dtype=self.precision))\n",
        "        Yobserved = (tf.squeeze(Y[:, self.observation_dimension, :])).numpy()\n",
        "\n",
        "        for ll in range(self.batch_size):\n",
        "            tseries = Yobserved[ll, :]\n",
        "            tcol = tseries[:nobs]\n",
        "            trow = tseries[(nobs - 1):]\n",
        "            hmat = np.flipud(sp.linalg.toeplitz(tcol[::-1], trow))\n",
        "            gm[:, ll * (winsize - 1):(ll + 1) * (winsize - 1)].assign(hmat[:, :-1])\n",
        "            gp[:, ll * (winsize - 1):(ll + 1) * (winsize - 1)].assign(hmat[:, 1:])\n",
        "\n",
        "        sig, U, V = tf.linalg.svd(gm, compute_uv=True, full_matrices=False)\n",
        "        sig_inv = tf.linalg.diag(1.0 / sig)\n",
        "        Uh = tf.linalg.adjoint(U)\n",
        "        A = gp @ V @ sig_inv @ Uh\n",
        "        evals, evecs = tf.linalg.eig(A)\n",
        "        phi = tf.linalg.solve(evecs, tf.cast(gm, dtype=self.precision_complex))\n",
        "\n",
        "        # Build reconstruction\n",
        "        phiinit = phi[:, ::(self.window-1)]\n",
        "        initconds = tf.cast(tf.transpose(tf.squeeze(Y[:, :, 0])), dtype=self.precision_complex)\n",
        "        sigp, Up, Vp = tf.linalg.svd(phiinit, compute_uv=True, full_matrices=False)\n",
        "        sigp_inv = tf.cast(tf.linalg.diag(1.0 / sigp), dtype=self.precision_complex)\n",
        "        kmat = initconds @ Vp @ sigp_inv @ tf.linalg.adjoint(Up)\n",
        "        recon = tf.reshape(tf.transpose(tf.math.real(kmat @ phi)), [self.batch_size, self.window-1, self.phys_dim])\n",
        "        return recon, evals, evecs, phi\n",
        "\n",
        "    def get_config(self):\n",
        "        base_config = super().get_config()\n",
        "        return {**base_config,\n",
        "                'encoder': self.encoder,\n",
        "                'decoder': self.decoder}\n"
      ],
      "metadata": {
        "id": "IdrPZrp8W_bP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "    Author:\n",
        "        Jay Lago, NIWC/SDSU, 2021\n",
        "\"\"\"\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.losses import MSE\n",
        "\n",
        "\n",
        "class LossDLDMD(keras.losses.Loss):\n",
        "    def __init__(self, hyp_params, **kwargs):\n",
        "        super(LossDLDMD, self).__init__(**kwargs)\n",
        "\n",
        "        # Parameters\n",
        "        self.a1 = hyp_params['a1']\n",
        "        self.a2 = hyp_params['a2']\n",
        "        self.a3 = hyp_params['a3']\n",
        "        self.a4 = hyp_params['a4']\n",
        "        self.precision = hyp_params['precision']\n",
        "        self.num_time_steps = int(hyp_params['num_time_steps'])\n",
        "        self.num_observables = hyp_params['num_observables']\n",
        "        self.window = self.num_time_steps - (self.num_observables - 1)\n",
        "\n",
        "        # Loss components\n",
        "        self.loss_recon = tf.constant(0.0, dtype=self.precision)\n",
        "        self.loss_pred = tf.constant(0.0, dtype=self.precision)\n",
        "        self.loss_dmd = tf.constant(0.0, dtype=self.precision)\n",
        "        self.loss_reg = tf.constant(0.0, dtype=self.precision)\n",
        "        self.total_loss = tf.constant(0.0, dtype=self.precision)\n",
        "\n",
        "    def call(self, model, obs):\n",
        "        \"\"\"\n",
        "            model = [y, x_ae, x_adv, y_adv_real, weights, evals, evecs, phi]\n",
        "        \"\"\"\n",
        "        y = tf.identity(model[0])\n",
        "        x_ae = tf.identity(model[1])\n",
        "        x_adv = tf.identity(model[2])\n",
        "        weights = model[4]\n",
        "        pred_horizon = -1\n",
        "        obs_windowed = obs[:, :(self.window-1), :]\n",
        "        # Autoencoder reconstruction\n",
        "        self.loss_recon = tf.reduce_mean(MSE(obs_windowed, x_ae))\n",
        "\n",
        "        # DMD reconstruction in the latent space\n",
        "        self.loss_dmd = self.dmdloss(y)\n",
        "\n",
        "        # Future state prediction\n",
        "        self.loss_pred = tf.reduce_mean(MSE(obs_windowed[:, :pred_horizon, :], x_adv[:, :pred_horizon, :]))\n",
        "\n",
        "        # Regularization on weights\n",
        "        self.loss_reg = tf.add_n([tf.nn.l2_loss(w) for w in weights])\n",
        "\n",
        "        # Total loss\n",
        "        self.total_loss = self.a1 * self.loss_recon + self.a2 * self.loss_dmd + \\\n",
        "                          self.a3 * self.loss_pred + self.a4 * self.loss_reg\n",
        "\n",
        "        return self.total_loss\n",
        "\n",
        "    @tf.function\n",
        "    def dmdloss(self, y):\n",
        "        y_m = tf.transpose(y, perm=[0, 2, 1])[:, :, :-1]\n",
        "        y_p = tf.transpose(y, perm=[0, 2, 1])[:, :, 1:]\n",
        "        [_, _, V] = tf.linalg.svd(y_m, compute_uv=True, full_matrices=False)\n",
        "        VVh = V @ tf.linalg.adjoint(V)\n",
        "        eye_mat = tf.eye(VVh.shape[-1], batch_shape=[VVh.shape[0]], dtype=self.precision)\n",
        "        return tf.reduce_mean(tf.norm(y_p @ (eye_mat - VVh), ord='fro', axis=[-2, -1]))\n",
        "\n",
        "    def get_config(self):\n",
        "        base_config = super().get_config()\n",
        "        return {**base_config,\n",
        "                'loss_recon': self.loss_recon,\n",
        "                'loss_pred': self.loss_pred,\n",
        "                'loss_dmd': self.loss_dmd,\n",
        "                'loss_inf': self.loss_inf,\n",
        "                'loss_reg': self.loss_reg,\n",
        "                'total_loss': self.total_loss}\n"
      ],
      "metadata": {
        "id": "fguOupqdceeE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "    Author:\n",
        "        Jay Lago, SDSU, 2021\n",
        "\"\"\"\n",
        "import tensorflow as tf\n",
        "import pickle\n",
        "import datetime as dt\n",
        "import os\n",
        "import sys\n",
        "sys.path.insert(0, '../../')\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# Setup\n",
        "# ==============================================================================\n",
        "NUM_SAVES = 1       # Number of times to save the model throughout training\n",
        "NUM_PLOTS = 100      # Number of diagnostic plots to generate while training\n",
        "DEVICE = '/GPU:1'\n",
        "GPUS = tf.config.experimental.list_physical_devices('GPU')\n",
        "if GPUS:\n",
        "    try:\n",
        "        for gpu in GPUS:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "    except RuntimeError as e:\n",
        "        print(e)\n",
        "else:\n",
        "    DEVICE = '/CPU:0'\n",
        "\n",
        "tf.keras.backend.set_floatx('float64')  # !! Set precision for the entire model here\n",
        "print(\"TensorFlow version: {}\".format(tf.__version__))\n",
        "print(\"Eager execution: {}\".format(tf.executing_eagerly()))\n",
        "print(\"Num GPUs available: {}\".format(len(GPUS)))\n",
        "print(\"Training at precision: {}\".format(tf.keras.backend.floatx()))\n",
        "print(\"Training on device: {}\".format(DEVICE))\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# Initialize hyper-parameters and Koopman model\n",
        "# ==============================================================================\n",
        "# General parameters\n",
        "hyp_params = dict()\n",
        "hyp_params['sim_start'] = dt.datetime.now().strftime(\"%Y-%m-%d-%H%M\")\n",
        "hyp_params['experiment'] = 'duffing'\n",
        "hyp_params['plot_path'] = './training_results/' + hyp_params['experiment'] + '_' + hyp_params['sim_start']\n",
        "hyp_params['model_path'] = './trained_models/' + hyp_params['experiment'] + '_' + hyp_params['sim_start']\n",
        "hyp_params['device'] = DEVICE\n",
        "hyp_params['precision'] = tf.keras.backend.floatx()\n",
        "hyp_params['num_init_conds'] = 15000\n",
        "hyp_params['num_train_init_conds'] = 10000\n",
        "hyp_params['num_val_init_conds'] = 3000\n",
        "hyp_params['num_test_init_conds'] = 2000\n",
        "hyp_params['time_final'] = 20\n",
        "hyp_params['delta_t'] = 0.05\n",
        "hyp_params['num_time_steps'] = int(hyp_params['time_final']/hyp_params['delta_t'] + 1)\n",
        "hyp_params['num_pred_steps'] = hyp_params['num_time_steps']\n",
        "hyp_params['max_epochs'] = 100\n",
        "hyp_params['save_every'] = hyp_params['max_epochs'] // NUM_SAVES\n",
        "hyp_params['plot_every'] = hyp_params['max_epochs'] // NUM_PLOTS\n",
        "\n",
        "# Hankel-DMD Window Size and Threshold\n",
        "hyp_params['num_observables'] = 40\n",
        "hyp_params['threshold'] = 6\n",
        "hyp_params['observation_dimension'] = 0\n",
        "\n",
        "# Universal network layer parameters (AE & Aux)\n",
        "hyp_params['optimizer'] = 'adam'\n",
        "hyp_params['batch_size'] = 128\n",
        "hyp_params['phys_dim'] = 2\n",
        "hyp_params['latent_dim'] = 2\n",
        "hyp_params['hidden_activation'] = tf.keras.activations.relu\n",
        "hyp_params['bias_initializer'] = tf.keras.initializers.Zeros\n",
        "\n",
        "# Encoding/Decoding Layer Parameters\n",
        "hyp_params['num_en_layers'] = 3\n",
        "hyp_params['num_en_neurons'] = 128\n",
        "hyp_params['kernel_init_enc'] = tf.keras.initializers.TruncatedNormal(mean=0.0, stddev=0.1)\n",
        "hyp_params['kernel_init_dec'] = tf.keras.initializers.TruncatedNormal(mean=0.0, stddev=0.1)\n",
        "hyp_params['ae_output_activation'] = tf.keras.activations.linear\n",
        "\n",
        "# Loss Function Parameters\n",
        "hyp_params['a1'] = tf.constant(1, dtype=hyp_params['precision'])        # Reconstruction\n",
        "hyp_params['a2'] = tf.constant(1, dtype=hyp_params['precision'])        # DMD\n",
        "hyp_params['a3'] = tf.constant(1, dtype=hyp_params['precision'])        # Prediction\n",
        "hyp_params['a4'] = tf.constant(1e-14, dtype=hyp_params['precision'])    # L-2 on weights\n",
        "\n",
        "# Learning rate\n",
        "hyp_params['lr'] = 1e-3\n",
        "\n",
        "# Initialize the Koopman model and loss\n",
        "myMachine = HDMD(hyp_params)\n",
        "myLoss = LossDLDMD(hyp_params)\n",
        "\n",
        "# ==============================================================================\n",
        "# Generate / load data\n",
        "# ==============================================================================\n",
        "data_fname = 'duffing_data.pkl'\n",
        "if os.path.exists(data_fname):\n",
        "    # Load data from file\n",
        "    data = pickle.load(open(data_fname, 'rb'))\n",
        "    data = tf.cast(data, dtype=hyp_params['precision'])\n",
        "else:\n",
        "    # Create new data\n",
        "    data = data_maker_duffing(x_lower1=-1, x_upper1=1, x_lower2=-1, x_upper2=1,\n",
        "                                  n_ic=hyp_params['num_init_conds'], dt=hyp_params['delta_t'],\n",
        "                                  tf=hyp_params['time_final'])\n",
        "    data = tf.cast(data[:, :, :2], dtype=hyp_params['precision'])\n",
        "    # Save data to file\n",
        "    pickle.dump(data, open(data_fname, 'wb'))\n",
        "\n",
        "# Create training and validation datasets from the initial conditions\n",
        "shuffled_data = tf.random.shuffle(data)\n",
        "ntic = hyp_params['num_train_init_conds']\n",
        "nvic = hyp_params['num_val_init_conds']\n",
        "train_data = shuffled_data[:ntic, :, :]\n",
        "val_data = shuffled_data[ntic:ntic+nvic, :, :]\n",
        "test_data = shuffled_data[ntic+nvic:, :, :]\n",
        "pickle.dump(train_data, open('data_train.pkl', 'wb'))\n",
        "pickle.dump(val_data, open('data_val.pkl', 'wb'))\n",
        "pickle.dump(test_data, open('data_test.pkl', 'wb'))\n",
        "train_data = tf.data.Dataset.from_tensor_slices(train_data)\n",
        "val_data = tf.data.Dataset.from_tensor_slices(val_data)\n",
        "test_data = tf.data.Dataset.from_tensor_slices(test_data)\n",
        "\n",
        "# Batch and prefetch the validation data to the GPUs\n",
        "val_set = val_data.batch(hyp_params['batch_size'], drop_remainder=True)\n",
        "# val_set = val_set.prefetch(tf.data.AUTOTUNE)\n",
        "try:\n",
        "    val_set = val_set.prefetch(tf.data.AUTOTUNE)\n",
        "except:\n",
        "    val_set = val_set.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "# ==============================================================================\n",
        "# Train the model\n",
        "# ==============================================================================\n",
        "results = train_model(hyp_params=hyp_params, train_data=train_data,\n",
        "                         val_set=val_set, model=myMachine, loss=myLoss)\n",
        "print(results['model'].summary())\n",
        "exit()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWSoHQTmXC9q",
        "outputId": "c86b479c-8912-4061-f20c-93f4768c4650"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.7.0\n",
            "Eager execution: True\n",
            "Num GPUs available: 1\n",
            "Training at precision: float64\n",
            "Training on device: /GPU:1\n",
            "Epoch 1 of 100 / Train -0.3807334 / Val -0.9341086 / LR 0.0010000 / 74.36 seconds\n",
            "Epoch 2 of 100 / Train -0.9590290 / Val -1.1373218 / LR 0.0010000 / 69.69 seconds\n",
            "Epoch 3 of 100 / Train -1.0605232 / Val -1.3260975 / LR 0.0010000 / 69.54 seconds\n",
            "Epoch 4 of 100 / Train -1.1222725 / Val -1.2017841 / LR 0.0010000 / 69.92 seconds\n",
            "Epoch 5 of 100 / Train -1.1337570 / Val -1.2103693 / LR 0.0010000 / 69.51 seconds\n",
            "Epoch 6 of 100 / Train -1.1549959 / Val -1.2884052 / LR 0.0010000 / 69.61 seconds\n",
            "Epoch 7 of 100 / Train -1.2113872 / Val -1.3241477 / LR 0.0010000 / 69.18 seconds\n",
            "Epoch 8 of 100 / Train -1.2201081 / Val -1.4912920 / LR 0.0010000 / 69.61 seconds\n",
            "Epoch 9 of 100 / Train -1.2312653 / Val -1.3236756 / LR 0.0010000 / 69.68 seconds\n",
            "Epoch 10 of 100 / Train -1.2334696 / Val -1.4756802 / LR 0.0010000 / 69.79 seconds\n",
            "Epoch 11 of 100 / Train -1.2262324 / Val -1.6568995 / LR 0.0010000 / 69.88 seconds\n",
            "Epoch 12 of 100 / Train -1.2290360 / Val -1.5323510 / LR 0.0010000 / 69.65 seconds\n",
            "Epoch 13 of 100 / Train -1.2781463 / Val -1.4251514 / LR 0.0010000 / 68.98 seconds\n",
            "Epoch 14 of 100 / Train -1.3056750 / Val -1.2015874 / LR 0.0010000 / 69.84 seconds\n",
            "Epoch 15 of 100 / Train -1.3234781 / Val -1.3744593 / LR 0.0010000 / 69.86 seconds\n",
            "Epoch 16 of 100 / Train -1.3031408 / Val -1.6304978 / LR 0.0010000 / 69.87 seconds\n",
            "Epoch 17 of 100 / Train -1.3342831 / Val -1.5135755 / LR 0.0010000 / 69.67 seconds\n",
            "Epoch 18 of 100 / Train -1.3343475 / Val -1.4169376 / LR 0.0010000 / 70.30 seconds\n",
            "Epoch 19 of 100 / Train -1.3385564 / Val -1.4559698 / LR 0.0010000 / 69.55 seconds\n",
            "Epoch 20 of 100 / Train -1.3501712 / Val -1.3843819 / LR 0.0010000 / 70.10 seconds\n",
            "Epoch 21 of 100 / Train -1.3412361 / Val -1.4662918 / LR 0.0010000 / 69.45 seconds\n",
            "Epoch 22 of 100 / Train -1.3834535 / Val -1.5462766 / LR 0.0010000 / 91.96 seconds\n",
            "Epoch 23 of 100 / Train -1.3519269 / Val -1.2730186 / LR 0.0010000 / 69.78 seconds\n",
            "Epoch 24 of 100 / Train -1.3857017 / Val -1.7295569 / LR 0.0010000 / 69.67 seconds\n",
            "Epoch 25 of 100 / Train -1.3527550 / Val -1.3582625 / LR 0.0010000 / 69.87 seconds\n",
            "Epoch 26 of 100 / Train -1.3952476 / Val -1.7570906 / LR 0.0010000 / 69.62 seconds\n",
            "Epoch 27 of 100 / Train -1.3966779 / Val -1.3148135 / LR 0.0010000 / 69.55 seconds\n",
            "Epoch 28 of 100 / Train -1.3704046 / Val -1.5740281 / LR 0.0010000 / 69.73 seconds\n",
            "Epoch 29 of 100 / Train -1.3412759 / Val -1.4593836 / LR 0.0010000 / 69.72 seconds\n",
            "Epoch 30 of 100 / Train -1.3651449 / Val -1.5713246 / LR 0.0010000 / 69.60 seconds\n",
            "Epoch 31 of 100 / Train -1.3655543 / Val -1.5943540 / LR 0.0010000 / 69.59 seconds\n",
            "Epoch 32 of 100 / Train -1.3734789 / Val -1.5464176 / LR 0.0010000 / 70.01 seconds\n",
            "Epoch 33 of 100 / Train -1.4004579 / Val -1.6419376 / LR 0.0010000 / 70.87 seconds\n",
            "Epoch 34 of 100 / Train -1.4077146 / Val -1.6573120 / LR 0.0010000 / 69.72 seconds\n",
            "Epoch 35 of 100 / Train -1.3706643 / Val -1.5032257 / LR 0.0010000 / 69.74 seconds\n",
            "Epoch 36 of 100 / Train -1.3384448 / Val -1.6044521 / LR 0.0010000 / 69.67 seconds\n",
            "Epoch 37 of 100 / Train -1.3094864 / Val -1.2098189 / LR 0.0010000 / 70.33 seconds\n",
            "Epoch 38 of 100 / Train -1.2655648 / Val -1.4800561 / LR 0.0010000 / 69.71 seconds\n",
            "Epoch 39 of 100 / Train -1.3196199 / Val -1.2534734 / LR 0.0010000 / 69.44 seconds\n",
            "Epoch 40 of 100 / Train -1.3269119 / Val -1.3640476 / LR 0.0010000 / 69.50 seconds\n",
            "Epoch 41 of 100 / Train -1.2982701 / Val -1.3083660 / LR 0.0010000 / 92.07 seconds\n",
            "Epoch 42 of 100 / Train -1.3529016 / Val -1.3124232 / LR 0.0010000 / 69.33 seconds\n",
            "Epoch 43 of 100 / Train -1.3284155 / Val -1.2501282 / LR 0.0010000 / 69.16 seconds\n",
            "Epoch 44 of 100 / Train -1.3021998 / Val -1.4378247 / LR 0.0010000 / 69.34 seconds\n",
            "Epoch 45 of 100 / Train -1.3202889 / Val -1.2245545 / LR 0.0010000 / 69.78 seconds\n",
            "Epoch 46 of 100 / Train -1.3181930 / Val -1.4221267 / LR 0.0010000 / 69.55 seconds\n",
            "Epoch 47 of 100 / Train -1.2822499 / Val -1.6200757 / LR 0.0010000 / 69.46 seconds\n",
            "Epoch 48 of 100 / Train -1.3078892 / Val -1.5133469 / LR 0.0010000 / 69.72 seconds\n",
            "Epoch 49 of 100 / Train -1.3663935 / Val -1.4093396 / LR 0.0010000 / 69.73 seconds\n",
            "Epoch 50 of 100 / Train -1.3472375 / Val -1.2761068 / LR 0.0010000 / 69.30 seconds\n",
            "Epoch 51 of 100 / Train -1.3194902 / Val -1.3278773 / LR 0.0010000 / 69.35 seconds\n",
            "Epoch 52 of 100 / Train -1.3166203 / Val -1.1921311 / LR 0.0010000 / 69.11 seconds\n",
            "Epoch 53 of 100 / Train -1.3420439 / Val -1.4977381 / LR 0.0010000 / 69.47 seconds\n",
            "Epoch 54 of 100 / Train -1.3111233 / Val -1.5809096 / LR 0.0010000 / 69.11 seconds\n",
            "Epoch 55 of 100 / Train -1.4160182 / Val -1.2145414 / LR 0.0010000 / 69.60 seconds\n",
            "Epoch 56 of 100 / Train -1.4240306 / Val -1.3511958 / LR 0.0010000 / 69.33 seconds\n",
            "Epoch 57 of 100 / Train -1.4179637 / Val -1.4763826 / LR 0.0010000 / 69.79 seconds\n",
            "Epoch 58 of 100 / Train -1.3956880 / Val -1.4257386 / LR 0.0010000 / 68.78 seconds\n",
            "Epoch 59 of 100 / Train -1.3998696 / Val -1.3305388 / LR 0.0010000 / 68.17 seconds\n",
            "Epoch 60 of 100 / Train -1.4235089 / Val -1.4489697 / LR 0.0010000 / 67.94 seconds\n",
            "Epoch 61 of 100 / Train -1.4813074 / Val -1.5928797 / LR 0.0010000 / 68.59 seconds\n",
            "Epoch 62 of 100 / Train -1.4746834 / Val -1.4819892 / LR 0.0010000 / 68.17 seconds\n",
            "Epoch 63 of 100 / Train -1.4615962 / Val -1.3247308 / LR 0.0010000 / 68.42 seconds\n",
            "Epoch 64 of 100 / Train -1.4306507 / Val -1.5550477 / LR 0.0010000 / 68.51 seconds\n",
            "Epoch 65 of 100 / Train -1.5090719 / Val -1.6786990 / LR 0.0010000 / 68.34 seconds\n",
            "Epoch 66 of 100 / Train -1.4451930 / Val -1.4506975 / LR 0.0010000 / 68.43 seconds\n",
            "Epoch 67 of 100 / Train -1.5097499 / Val -1.6796325 / LR 0.0010000 / 68.28 seconds\n",
            "Epoch 68 of 100 / Train -1.4871262 / Val -1.6024613 / LR 0.0010000 / 68.25 seconds\n",
            "Epoch 69 of 100 / Train -1.5402217 / Val -1.8616318 / LR 0.0010000 / 68.44 seconds\n",
            "Epoch 70 of 100 / Train -1.5099366 / Val -1.5199520 / LR 0.0010000 / 68.35 seconds\n",
            "Epoch 71 of 100 / Train -1.4990907 / Val -1.4837625 / LR 0.0010000 / 68.32 seconds\n",
            "Epoch 72 of 100 / Train -1.4878482 / Val -1.5967214 / LR 0.0010000 / 68.36 seconds\n",
            "Epoch 73 of 100 / Train -1.4978070 / Val -1.7638527 / LR 0.0010000 / 68.64 seconds\n",
            "Epoch 74 of 100 / Train -1.5143457 / Val -1.6947502 / LR 0.0010000 / 68.55 seconds\n",
            "Epoch 75 of 100 / Train -1.4464418 / Val -1.6719675 / LR 0.0010000 / 68.57 seconds\n",
            "Epoch 76 of 100 / Train -1.4702251 / Val -1.5352014 / LR 0.0010000 / 68.82 seconds\n",
            "Epoch 77 of 100 / Train -1.4640307 / Val -1.7353204 / LR 0.0010000 / 68.82 seconds\n",
            "Epoch 78 of 100 / Train -1.4949008 / Val -1.5752333 / LR 0.0010000 / 69.01 seconds\n",
            "Epoch 79 of 100 / Train -1.4798849 / Val -1.4022812 / LR 0.0010000 / 68.57 seconds\n",
            "Epoch 80 of 100 / Train -1.5048418 / Val -1.3529873 / LR 0.0010000 / 68.53 seconds\n",
            "Epoch 81 of 100 / Train -1.4977480 / Val -1.4724683 / LR 0.0010000 / 68.51 seconds\n",
            "Epoch 82 of 100 / Train -1.4513530 / Val -1.3976974 / LR 0.0010000 / 68.51 seconds\n",
            "Epoch 83 of 100 / Train -1.4908076 / Val -1.6552531 / LR 0.0010000 / 68.54 seconds\n",
            "Epoch 84 of 100 / Train -1.4715184 / Val -1.3929620 / LR 0.0010000 / 68.37 seconds\n",
            "Epoch 85 of 100 / Train -1.5044662 / Val -1.4146438 / LR 0.0010000 / 68.37 seconds\n",
            "Epoch 86 of 100 / Train -1.5297800 / Val -1.4369122 / LR 0.0010000 / 68.50 seconds\n",
            "Epoch 87 of 100 / Train -1.5036042 / Val -1.6501560 / LR 0.0010000 / 68.62 seconds\n",
            "Epoch 88 of 100 / Train -1.4640675 / Val -1.4788043 / LR 0.0010000 / 68.48 seconds\n",
            "Epoch 89 of 100 / Train -1.4831273 / Val -1.3988171 / LR 0.0010000 / 68.44 seconds\n",
            "Epoch 90 of 100 / Train -1.4745278 / Val -1.2763508 / LR 0.0010000 / 69.07 seconds\n",
            "Epoch 91 of 100 / Train -1.4618037 / Val -1.5229490 / LR 0.0010000 / 68.54 seconds\n",
            "Epoch 92 of 100 / Train -1.4243887 / Val -1.5245873 / LR 0.0010000 / 68.34 seconds\n",
            "Epoch 93 of 100 / Train -1.3891462 / Val -1.5048615 / LR 0.0010000 / 68.49 seconds\n",
            "Epoch 94 of 100 / Train -1.4571563 / Val -1.4816118 / LR 0.0010000 / 68.47 seconds\n",
            "Epoch 95 of 100 / Train -1.4996841 / Val -1.7341999 / LR 0.0010000 / 68.64 seconds\n",
            "Epoch 96 of 100 / Train -1.4222773 / Val -1.5518376 / LR 0.0010000 / 68.35 seconds\n",
            "Epoch 97 of 100 / Train -1.4365744 / Val -1.6178916 / LR 0.0010000 / 68.48 seconds\n",
            "Epoch 98 of 100 / Train -1.4214284 / Val -1.7077220 / LR 0.0010000 / 68.57 seconds\n",
            "Epoch 99 of 100 / Train -1.4272141 / Val -1.6979112 / LR 0.0010000 / 68.48 seconds\n",
            "Epoch 100 of 100 / Train -1.4401642 / Val -1.6977610 / LR 0.0010000 / 68.37 seconds\n",
            "\n",
            "Total training time: 123.99 minutes\n",
            "Final train loss: -1.4401642\n",
            "Final validation loss: -1.6977610\n",
            "Model: \"hdmd\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " encoder (Sequential)        (None, 401, 2)            50178     \n",
            "                                                                 \n",
            " decoder (Sequential)        (None, 361, 2)            50178     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 100,356\n",
            "Trainable params: 100,356\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    }
  ]
}
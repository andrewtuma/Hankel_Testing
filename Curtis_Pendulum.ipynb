{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Curtis_Pendulum.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNTa1nENyx5onBzJ36BtDK3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andrewtuma/Hankel_Testing/blob/main/Curtis_Pendulum.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "uQ06y1k6Wv2M",
        "outputId": "5dc1d46e-78c0-4cc2-84ae-742f4cace2b9"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "    Created by:\n",
        "        Opal Issan\n",
        "\n",
        "    Modified:\n",
        "        17 Nov 2020 - Jay Lago\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# Function Implementations\n",
        "# ==============================================================================\n",
        "def dyn_sys_discrete(lhs, mu=-0.05, lam=-1):\n",
        "    \"\"\" example 1:\n",
        "    ODE =>\n",
        "    dx1/dt = mu*x1\n",
        "    dx2/dt = lam*(x2-x1^2)\n",
        "\n",
        "    By default: mu =-0.05, and lambda = -1.\n",
        "    \"\"\"\n",
        "    rhs = np.zeros(2)\n",
        "    rhs[0] = mu * lhs[0]\n",
        "    rhs[1] = lam * (lhs[1] - (lhs[0]) ** 2.)\n",
        "    return rhs\n",
        "\n",
        "def dyn_sys_pendulum(lhs):\n",
        "    \"\"\" pendulum example:\n",
        "    ODE =>\n",
        "    dx1/dt = x2\n",
        "    dx2/dt = -sin(x1)\n",
        "    \"\"\"\n",
        "    rhs = np.zeros(2)\n",
        "    rhs[0] = lhs[1]\n",
        "    rhs[1] = -np.sin(lhs[0])\n",
        "    return rhs\n",
        "\n",
        "def dyn_sys_fluid(lhs, mu=0.1, omega=1, A=-0.1, lam=10):\n",
        "    \"\"\"fluid flow example:\n",
        "    ODE =>\n",
        "    dx1/dt = mu*x1 - omega*x2 + A*x1*x3\n",
        "    dx2/dt = omega*x1 + mu*x2 + A*x2*x3\n",
        "    dx3/dt = -lam(x3 - x1^2 - x2^2)\n",
        "    \"\"\"\n",
        "    rhs = np.zeros(3)\n",
        "    rhs[0] = mu * lhs[0] - omega * lhs[1] + A * lhs[0] * lhs[2]\n",
        "    rhs[1] = omega * lhs[0] + mu * lhs[1] + A * lhs[1] * lhs[2]\n",
        "    rhs[2] = -lam * (lhs[2] - lhs[0] ** 2 - lhs[1] ** 2)\n",
        "    return rhs\n",
        "\n",
        "def dyn_sys_kdv(lhs, a1=0, c=3):\n",
        "    \"\"\" planar kdv:\n",
        "    dx1/dt = x2\n",
        "    dx2/dt = a1 + c*x1 - 3*x2^2\n",
        "    \"\"\"\n",
        "    rhs = np.zeros(2)\n",
        "    rhs[0] = lhs[1]\n",
        "    rhs[1] = a1 + c*lhs[0] - 3*lhs[0]**2\n",
        "    return rhs\n",
        "\n",
        "def dyn_sys_duffing_driven(lhs, alpha=0.1, gamma=0.05, omega=1.1):\n",
        "    \"\"\" Duffing oscillator:\n",
        "    dx/dt = y\n",
        "    dy/dt = x - x^3 - gamma*y + alpha*cos(omega*t)\n",
        "    \"\"\"\n",
        "    rhs = np.zeros(3)\n",
        "    rhs[0] = lhs[1]\n",
        "    rhs[1] = lhs[0] - lhs[0]**3 - gamma*lhs[1] + alpha*np.cos(omega*lhs[2])\n",
        "    rhs[2] = lhs[2]\n",
        "    return rhs\n",
        "\n",
        "def dyn_sys_duffing(lhs):\n",
        "    \"\"\" Duffing oscillator:\n",
        "    dx/dt = y\n",
        "    dy/dt = x - x^3\n",
        "    \"\"\"\n",
        "    rhs = np.zeros(2)\n",
        "    rhs[0] = lhs[1]\n",
        "    rhs[1] = lhs[0] - lhs[0]**3\n",
        "    return rhs\n",
        "\n",
        "def dyn_sys_duffing_bollt(lhs, alpha=1.0, beta=-1.0, delta=0.5):\n",
        "    \"\"\" Duffing oscillator:\n",
        "    dx/dt = y\n",
        "    dy/dt = -delta*y - x*(beta + alpha*x^2)\n",
        "    \"\"\"\n",
        "    rhs = np.zeros(2)\n",
        "    rhs[0] = lhs[1]\n",
        "    rhs[1] = -delta*lhs[1] - lhs[0]*(beta + alpha*lhs[0]**2)\n",
        "    return rhs\n",
        "\n",
        "def rk4(lhs, dt, function):\n",
        "    \"\"\"\n",
        "    :param lhs: previous step state.\n",
        "    :param dt: delta t.\n",
        "    :param data_type: \"ex1\" or \"ex2\".\n",
        "    :return:  Runge–Kutta 4th order method.\n",
        "    \"\"\"\n",
        "    k1 = dt * function(lhs)\n",
        "    k2 = dt * function(lhs + k1 / 2.0)\n",
        "    k3 = dt * function(lhs + k2 / 2.0)\n",
        "    k4 = dt * function(lhs + k3)\n",
        "    rhs = lhs + 1.0 / 6.0 * (k1 + 2.0 * (k2 + k3) + k4)\n",
        "    return rhs\n",
        "\n",
        "def data_maker_discrete(x_lower1, x_upper1, x_lower2, x_upper2, n_ic=1e4, dt=0.02, tf=1.0, seed=None, testing=False):\n",
        "    \"\"\"\n",
        "    :param tf: final time. default is 15.\n",
        "    :param dt: delta t.\n",
        "    :param x_lower1: lower bound of x1, initial condition.\n",
        "    :param x_upper1: upper bound of x1, initial condition.\n",
        "    :param x_upper2: lower bound of x2, initial condition.\n",
        "    :param x_lower2: upper bound of x1, initial condition.\n",
        "    :param n_side: number of initial conditions on each axis. default is 100.\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    # set seed\n",
        "    np.random.seed(seed=seed)\n",
        "\n",
        "    # dim - time steps\n",
        "    nsteps = int(tf / dt)\n",
        "\n",
        "    # number of initial conditions.\n",
        "    n_ic = int(n_ic)\n",
        "\n",
        "    # create initial condition grid\n",
        "    if testing:\n",
        "        icond1 = np.linspace(x_lower1, x_upper1, 10)\n",
        "        icond2 = np.linspace(x_lower2, x_upper2, 2)\n",
        "        xx, yy = np.meshgrid(icond1, icond2)\n",
        "\n",
        "        # solve the system using Runge–Kutta 4th order method, see rk4 function above.\n",
        "        data_mat = np.zeros((n_ic, 2, nsteps + 1), dtype=np.float64)\n",
        "        ic = 0\n",
        "        for x1 in range(2):\n",
        "            for x2 in range(10):\n",
        "                data_mat[ic, :, 0] = np.array([xx[x1, x2], yy[x1, x2]], dtype=np.float64)\n",
        "                for jj in range(nsteps):\n",
        "                    data_mat[ic, :, jj + 1] = rk4(data_mat[ic, :, jj], dt, dyn_sys_discrete)\n",
        "                ic += 1\n",
        "    else:\n",
        "        icond1 = np.random.uniform(x_lower1, x_upper1, n_ic)\n",
        "        icond2 = np.random.uniform(x_lower2, x_upper2, n_ic)\n",
        "\n",
        "        # solve the system using Runge–Kutta 4th order method, see rk4 function above.\n",
        "        data_mat = np.zeros((n_ic, 2, nsteps + 1), dtype=np.float64)\n",
        "        for ii in range(n_ic):\n",
        "            data_mat[ii, :, 0] = np.array([icond1[ii], icond2[ii]], dtype=np.float64)\n",
        "            for jj in range(nsteps):\n",
        "                data_mat[ii, :, jj + 1] = rk4(data_mat[ii, :, jj], dt, dyn_sys_discrete)\n",
        "\n",
        "    return np.transpose(data_mat, [0, 2, 1])\n",
        "\n",
        "def data_maker_pendulum(x_lower1, x_upper1, x_lower2, x_upper2, n_ic=10000, dt=0.02, tf=1.0, seed=None):\n",
        "    \"\"\"\n",
        "    :param tf: final time. default is 15.\n",
        "    :param dt: delta t.\n",
        "    :param x_lower1: lower bound of x1, initial condition.\n",
        "    :param x_upper1: upper bound of x1, initial condition.\n",
        "    :param x_upper2: lower bound of x2, initial condition.\n",
        "    :param x_lower2: upper bound of x1, initial condition.\n",
        "    :param n_ic: number of initial conditions\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    # set seed\n",
        "    np.random.seed(seed=seed)\n",
        "\n",
        "    # dim - time steps\n",
        "    nsteps = int(tf / dt)\n",
        "\n",
        "    # number of initial conditions\n",
        "    n_ic = int(n_ic)\n",
        "\n",
        "    # create initial condition grid\n",
        "    rand_x1 = np.random.uniform(x_lower1, x_upper1, 100 * n_ic)\n",
        "    rand_x2 = np.random.uniform(x_lower2, x_upper2, 100 * n_ic)\n",
        "    max_potential = 0.99\n",
        "    potential = lambda x, y: (1 / 2) * y ** 2 - np.cos(x)\n",
        "    iconds = np.asarray([[x, y] for x, y in zip(rand_x1, rand_x2)\n",
        "                         if potential(x, y) <= max_potential])[:n_ic, :]\n",
        "\n",
        "    # solve the system using Runge–Kutta 4th order method, see rk4 function above\n",
        "    data_mat = np.zeros((n_ic, 2, nsteps + 1), dtype=np.float64)\n",
        "    for ii in range(n_ic):\n",
        "        data_mat[ii, :, 0] = np.array([iconds[ii, 0], iconds[ii, 1]], dtype=np.float64)\n",
        "        for jj in range(nsteps):\n",
        "            data_mat[ii, :, jj + 1] = rk4(data_mat[ii, :, jj], dt, dyn_sys_pendulum)\n",
        "\n",
        "    return np.transpose(data_mat, [0, 2, 1])\n",
        "\n",
        "def data_maker_pendulum_uniform(n_ic=10000, dt=0.02, tf=3.0, seed=None):\n",
        "    nsteps = np.int(tf / dt)\n",
        "    n_ic = np.int(n_ic)\n",
        "    rand_x1 = np.random.uniform(-3.1, 0, 100*n_ic)\n",
        "    rand_x2 = np.zeros((100*n_ic))\n",
        "    max_potential = 0.99\n",
        "    potential = lambda x, y: (1 / 2) * y ** 2 - np.cos(x)\n",
        "    iconds = np.asarray([[x, y] for x, y in zip(rand_x1, rand_x2)\n",
        "                         if potential(x, y) <= max_potential])[:n_ic, :]\n",
        "    data_mat = np.zeros((n_ic, 2, nsteps + 1), dtype=np.float64)\n",
        "    for ii in range(n_ic):\n",
        "        data_mat[ii, :, 0] = np.array([iconds[ii, 0], iconds[ii, 1]], dtype=np.float64)\n",
        "        for jj in range(nsteps):\n",
        "            data_mat[ii, :, jj + 1] = rk4(data_mat[ii, :, jj], dt, dyn_sys_pendulum)\n",
        "\n",
        "    return np.transpose(data_mat, [0, 2, 1])\n",
        "\n",
        "def data_maker_fluid_flow_slow(r_lower=0, r_upper=1.1, t_lower=0, t_upper=2*np.pi, n_ic=1e4, dt=0.05, tf=6, seed=None):\n",
        "    \"\"\"\n",
        "    :param r_lower: lower bound for r. Default is 0.\n",
        "    :param r_upper: Upper bound for r. Default is 1.\n",
        "    :param t_lower: Lower bound for theta. Default is 0.\n",
        "    :param t_upper: Upper bound for theta. Default is 2pi.\n",
        "    :param n_ic: number of initial conditions. Default is 10000.\n",
        "    :param dt: time step size. Default is 0.05.\n",
        "    :param tf: final time. default is 6.\n",
        "    :return: csv file\n",
        "    \"\"\"\n",
        "    # set seed\n",
        "    np.random.seed(seed=seed)\n",
        "\n",
        "    # dim - time steps\n",
        "    nsteps = int(tf / dt)\n",
        "\n",
        "    # number of initial conditions for slow manifold.\n",
        "    n_ic_slow = int(n_ic)\n",
        "\n",
        "    # create initial condition grid.\n",
        "    r = np.random.uniform(r_lower, r_upper, n_ic_slow)\n",
        "    theta = np.random.uniform(t_lower, t_upper, n_ic_slow)\n",
        "\n",
        "    # compute x1, x2, and x3, based on theta and r\n",
        "    x1 = r * np.cos(theta)\n",
        "    x2 = r * np.sin(theta)\n",
        "    x3 = np.power(x1, 2) + np.power(x2, 2)\n",
        "\n",
        "    # initialize initial conditions matrix.\n",
        "    iconds = np.zeros((n_ic_slow, 3))\n",
        "\n",
        "    # initial conditions for slow manifold.\n",
        "    iconds[:n_ic_slow] = np.asarray([[x, y, z] for x, y, z in zip(x1, x2, x3)])\n",
        "\n",
        "    # solve the system using Runge–Kutta 4th order method, see rk4 function above.\n",
        "    data_mat = np.zeros((n_ic_slow, 3, nsteps + 1), dtype=np.float64)\n",
        "    for ii in range(n_ic_slow):\n",
        "        data_mat[ii, :, 0] = np.array([iconds[ii, 0], iconds[ii, 1], iconds[ii, 2]], dtype=np.float64)\n",
        "        for jj in range(nsteps):\n",
        "            data_mat[ii, :, jj + 1] = rk4(data_mat[ii, :, jj], dt, dyn_sys_fluid)\n",
        "\n",
        "    return np.transpose(data_mat, [0, 2, 1])\n",
        "\n",
        "def data_maker_fluid_flow_full(x1_lower=-1.1, x1_upper=1.1, x2_lower=-1.1, x2_upper=1.1, x3_lower=0.0, x3_upper=2.43,\n",
        "                               n_ic=1e4, dt=0.05, tf=6, seed=None):\n",
        "    # set seed\n",
        "    np.random.seed(seed=seed)\n",
        "\n",
        "    # Number of time steps\n",
        "    nsteps = np.int(tf / dt)\n",
        "\n",
        "    # Number of initial conditions\n",
        "    n_ic = np.int(n_ic)\n",
        "\n",
        "    # Create initial condition grid\n",
        "    x1 = np.random.uniform(x1_lower, x1_upper, n_ic)\n",
        "    x2 = np.random.uniform(x2_lower, x2_upper, n_ic)\n",
        "    x3 = np.random.uniform(x3_lower, x3_upper, n_ic)\n",
        "\n",
        "    # Initialize initial conditions matrix\n",
        "    iconds = np.zeros((n_ic, 3))\n",
        "\n",
        "    # Initial conditions zip\n",
        "    iconds[:n_ic] = np.asarray([[x, y, z] for x, y, z in zip(x1, x2, x3)])\n",
        "\n",
        "    # Solve the system using Runge–Kutta 4th order method, see rk4 function above\n",
        "    data_mat = np.zeros((n_ic, 3, nsteps+1), dtype=np.float64)\n",
        "    for ii in range(n_ic):\n",
        "        data_mat[ii, :, 0] = np.array([iconds[ii, 0], iconds[ii, 1], iconds[ii, 2]], dtype=np.float64)\n",
        "        for jj in range(nsteps):\n",
        "            data_mat[ii, :, jj+1] = rk4(data_mat[ii, :, jj], dt, dyn_sys_fluid)\n",
        "\n",
        "    return np.transpose(data_mat, [0, 2, 1])\n",
        "\n",
        "def data_maker_kdv(x_lower1, x_upper1, x_lower2, x_upper2, n_ic=10000, dt=0.01, tf=1.0, seed=None):\n",
        "    # Setup\n",
        "    np.random.seed(seed=seed)\n",
        "    nsteps = int(tf / dt)\n",
        "    n_ic = int(n_ic)\n",
        "    # Generate initial conditions\n",
        "    icond1 = np.random.uniform(x_lower1, x_upper1, 10*n_ic)\n",
        "    icond2 = np.random.uniform(x_lower2, x_upper2, 10*n_ic)\n",
        "    n_try = 10*n_ic\n",
        "    # Integrate\n",
        "    data_mat = np.zeros((n_try, 2, nsteps+1), dtype=np.float64)\n",
        "    for ii in range(n_try):\n",
        "        data_mat[ii, :, 0] = np.array([icond1[ii], icond2[ii]], dtype=np.float64)\n",
        "        for jj in range(nsteps):\n",
        "            data_mat[ii, :, jj+1] = rk4(data_mat[ii, :, jj], dt, dyn_sys_kdv)\n",
        "            # if (data_mat[ii, 0, jj+1] < x_lower1 or data_mat[ii, 1, jj+1] > x_upper1\n",
        "            #         or data_mat[ii, 1, jj+1] < x_lower2 or data_mat[ii, 1, jj+1] > x_upper2):\n",
        "            #     break\n",
        "    accept = np.abs(data_mat[:, 0, -1]) < 3\n",
        "    data_mat = data_mat[accept, :, :]\n",
        "    accept = np.abs(data_mat[:, 1, -1]) < 3\n",
        "    data_mat = data_mat[accept, :, :]\n",
        "    data_mat = data_mat[:n_ic, :, :]\n",
        "    return np.transpose(data_mat, [0, 2, 1])\n",
        "\n",
        "def data_maker_duffing_driven(x_lower1, x_upper1, x_lower2, x_upper2, n_ic=10000, dt=0.01, tf=1.0, seed=None):\n",
        "    # Setup\n",
        "    np.random.seed(seed=seed)\n",
        "    nsteps = int(tf / dt)\n",
        "    n_ic = int(n_ic)\n",
        "    # Generate initial conditions\n",
        "    icond1 = np.random.uniform(x_lower1, x_upper1, n_ic)\n",
        "    icond2 = np.random.uniform(x_lower2, x_upper2, n_ic)\n",
        "    # Integrate\n",
        "    data_mat = np.zeros((n_ic, 3, nsteps+1), dtype=np.float64)\n",
        "    for ii in range(n_ic):\n",
        "        data_mat[ii, :2, 0] = np.array([icond1[ii], icond2[ii]], dtype=np.float64)\n",
        "        for jj in range(nsteps):\n",
        "            data_mat[ii, :, jj+1] = rk4(data_mat[ii, :, jj], dt, dyn_sys_duffing_driven)\n",
        "            data_mat[ii, 2, jj+1] = data_mat[ii, 2, jj] + dt\n",
        "    return np.transpose(data_mat, [0, 2, 1])\n",
        "\n",
        "def data_maker_duffing(x_lower1, x_upper1, x_lower2, x_upper2, n_ic=10000, dt=0.01, tf=1.0, seed=None):\n",
        "    # Setup\n",
        "    np.random.seed(seed=seed)\n",
        "    nsteps = int(tf / dt)\n",
        "    n_ic = int(n_ic)\n",
        "    # Generate initial conditions\n",
        "    icond1 = np.random.uniform(x_lower1, x_upper1, n_ic)\n",
        "    icond2 = np.random.uniform(x_lower2, x_upper2, n_ic)\n",
        "    # Integrate\n",
        "    data_mat = np.zeros((n_ic, 2, nsteps+1), dtype=np.float64)\n",
        "    for ii in range(n_ic):\n",
        "        data_mat[ii, :, 0] = np.array([icond1[ii], icond2[ii]], dtype=np.float64)\n",
        "        for jj in range(nsteps):\n",
        "            data_mat[ii, :, jj+1] = rk4(data_mat[ii, :, jj], dt, dyn_sys_duffing)\n",
        "    return np.transpose(data_mat, [0, 2, 1])\n",
        "\n",
        "def data_maker_duffing_bollt(x_lower1, x_upper1, x_lower2, x_upper2, n_ic=10000, dt=0.01, tf=1.0, seed=None):\n",
        "    # Setup\n",
        "    np.random.seed(seed=seed)\n",
        "    nsteps = int(tf / dt)\n",
        "    n_ic = int(n_ic)\n",
        "    # Generate initial conditions\n",
        "    icond1 = np.random.uniform(x_lower1, x_upper1, n_ic)\n",
        "    icond2 = np.random.uniform(x_lower2, x_upper2, n_ic)\n",
        "    # Integrate\n",
        "    data_mat = np.zeros((n_ic, 2, nsteps+1), dtype=np.float64)\n",
        "    for ii in range(n_ic):\n",
        "        data_mat[ii, :, 0] = np.array([icond1[ii], icond2[ii]], dtype=np.float64)\n",
        "        for jj in range(nsteps):\n",
        "            data_mat[ii, :, jj+1] = rk4(data_mat[ii, :, jj], dt, dyn_sys_duffing_bollt)\n",
        "    return np.transpose(data_mat, [0, 2, 1])\n",
        "\n",
        "# ==============================================================================\n",
        "# Test program\n",
        "# ==============================================================================\n",
        "if __name__ == \"__main__\":\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    create_discrete = False\n",
        "    create_pendulum = False\n",
        "    create_fluid_flow_slow = False\n",
        "    create_fluid_flow_full = False\n",
        "    create_kdv = False\n",
        "    create_duffing = True\n",
        "\n",
        "    if create_discrete:\n",
        "        # Generate the data\n",
        "        data = data_maker_discrete(x_lower1=-0.5, x_upper1=0.5, x_lower2=-0.5, x_upper2=0.5, n_ic=20, dt=0.02, tf=10)\n",
        "        # Visualize\n",
        "        plt.figure(1, figsize=(8, 8))\n",
        "        for ii in range(data.shape[0]):\n",
        "            plt.plot(data[ii, :, 0], data[ii, :, 1], '-')\n",
        "        plt.xlabel(\"x1\", fontsize=18)\n",
        "        plt.ylabel(\"X2\", fontsize=18)\n",
        "        plt.title(\"Discrete dataset\", fontsize=18)\n",
        "\n",
        "    if create_pendulum:\n",
        "        # Generate the data\n",
        "        data = data_maker_pendulum(x_lower1=-3.1, x_upper1=3.1, x_lower2=-2, x_upper2=2, n_ic=20, dt=0.02, tf=20)\n",
        "        # Visualize\n",
        "        plt.figure(2, figsize=(8, 8))\n",
        "        for ii in range(data.shape[0]):\n",
        "            plt.plot(data[ii, :, 0], data[ii, :, 1], '-')\n",
        "        plt.xlabel(\"x1\", fontsize=18)\n",
        "        plt.ylabel(\"X2\", fontsize=18)\n",
        "        plt.title(\"Pendulum dataset\", fontsize=18)\n",
        "\n",
        "    if create_fluid_flow_slow:\n",
        "        # Generate the data\n",
        "        data = data_maker_fluid_flow_slow(r_lower=0, r_upper=1.1, t_lower=0, t_upper=2*np.pi, n_ic=20, dt=0.05, tf=10)\n",
        "        # Visualize\n",
        "        fig = plt.figure(3, figsize=(8, 8))\n",
        "        ax = plt.axes(projection='3d')\n",
        "        for ii in range(data.shape[0]):\n",
        "            ax.plot3D(data[ii, :, 0], data[ii, :, 1], data[ii, :, 2])\n",
        "        ax.set_xlabel(\"$x_{1}$\", fontsize=18)\n",
        "        ax.set_ylabel(\"$x_{2}$\", fontsize=18)\n",
        "        ax.set_zlabel(\"$x_{3}$\", fontsize=18)\n",
        "        plt.title(\"Fluid Flow dataset\", fontsize=20)\n",
        "\n",
        "    if create_fluid_flow_full:\n",
        "        # Generate the data\n",
        "        data = data_maker_fluid_flow_full(x1_lower=-1.1, x1_upper=1.1, x2_lower=-1.1, x2_upper=1.1,\n",
        "                                          x3_lower=0.0, x3_upper=2.43, n_ic=20, dt=0.05, tf=6)\n",
        "        # Visualize\n",
        "        fig = plt.figure(4, figsize=(8, 8))\n",
        "        ax = plt.axes(projection='3d')\n",
        "        for ii in range(data.shape[0]):\n",
        "            ax.plot3D(data[ii, :, 0], data[ii, :, 1], data[ii, :, 2])\n",
        "        ax.set_xlabel(\"$x_{1}$\", fontsize=18)\n",
        "        ax.set_ylabel(\"$x_{2}$\", fontsize=18)\n",
        "        ax.set_zlabel(\"$x_{3}$\", fontsize=18)\n",
        "        plt.title(\"Fluid Flow dataset\", fontsize=20)\n",
        "\n",
        "    if create_kdv:\n",
        "        # Generate the data\n",
        "        data = data_maker_kdv(x_lower1=-2, x_upper1=2, x_lower2=-2, x_upper2=2, n_ic=1000, dt=0.01, tf=20)\n",
        "        # Visualize\n",
        "        plt.figure(2, figsize=(8, 8))\n",
        "        for ii in range(data.shape[0]):\n",
        "            npts = np.sum(np.abs(data[ii, :, 0]) > 0)\n",
        "            plt.plot(data[ii, :npts, 0], data[ii, :npts, 1], 'r-', lw=0.25)\n",
        "        plt.xlabel(\"x1\", fontsize=18)\n",
        "        plt.ylabel(\"X2\", fontsize=18)\n",
        "        plt.title(\"KdV dataset\", fontsize=18)\n",
        "\n",
        "    if create_duffing:\n",
        "        # Generate the data\n",
        "        data = data_maker_duffing(x_lower1=-1, x_upper1=1, x_lower2=-1, x_upper2=1, n_ic=2, dt=0.05, tf=200)\n",
        "        # Visualize\n",
        "        plt.figure(2, figsize=(8, 8))\n",
        "        plt.plot(data[0, :, 0], data[0, :, 1], 'r-', lw=0.5)\n",
        "        plt.plot(data[1, :, 0], data[1, :, 1], 'b-', lw=0.5)\n",
        "        plt.xlabel(\"x1\", fontsize=18)\n",
        "        plt.ylabel(\"X2\", fontsize=18)\n",
        "        plt.title(\"Duffing oscillator\", fontsize=18)\n",
        "\n",
        "    plt.show()\n",
        "    print(\"done\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "    Created by:\n",
        "        Jay Lago\n",
        "\n",
        "    Modified:\n",
        "        17 Nov 2020 - Jay Lago\n",
        "\"\"\"\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import matplotlib\n",
        "import numpy as np\n",
        "\n",
        "font = {'family': 'DejaVu Sans', 'size': 18}\n",
        "matplotlib.rc('font', **font)\n",
        "\n",
        "\n",
        "def diagnostic_plot(y_pred, y_true, hyp_params, epoch, save_path, loss_comps, val_loss):\n",
        "    if hyp_params['experiment'] == 'discrete' or \\\n",
        "            hyp_params['experiment'] == 'pendulum' or \\\n",
        "            hyp_params['experiment'] == 'van_der_pol' or \\\n",
        "            hyp_params['experiment'] == 'duffing' or \\\n",
        "            hyp_params['experiment'] == 'kdv':\n",
        "        plot_2D(y_pred, y_true, hyp_params, epoch, save_path, loss_comps, val_loss)\n",
        "    elif hyp_params['experiment'] == 'fluid_flow_slow' or \\\n",
        "            hyp_params['experiment'] == 'fluid_flow_full':\n",
        "        plot_3d(y_pred, y_true, hyp_params, epoch, save_path, loss_comps, val_loss)\n",
        "    else:\n",
        "        print(\"unknown experiment, create new diagnostic plots\")\n",
        "\n",
        "def net_steps_plot(num_steps):\n",
        "    \n",
        "    plt.plot(num_steps)\n",
        "    plt.title(\"Delay Path\")\n",
        "    plt.ylabel(\"Tau\")\n",
        "    plt.xlabel(\"batch #\")\n",
        "    plt.savefig(\"num_steps.png\")\n",
        "    plt.close()\n",
        "        \n",
        "def loss_diff_plot(loss_diff):\n",
        "    plt.plot(np.abs(loss_diff))\n",
        "    plt.title(\"Difference in step loss\")\n",
        "    plt.ylabel(\"Loss Difference\")\n",
        "    plt.yscale(\"log\")\n",
        "    plt.savefig(\"val_loss_dif.png\")\n",
        "    plt.close()\n",
        "        \n",
        "        \n",
        "def plot_2D(y_pred, y_true, hyp_params, epoch, save_path, loss_comps, val_loss):\n",
        "    enc = y_pred[0]\n",
        "    enc_dec = y_pred[1]\n",
        "    enc_adv_dec = y_pred[2]\n",
        "    enc_adv = y_pred[3]\n",
        "\n",
        "    fig, ax = plt.subplots(nrows=3, ncols=3, sharex=False, sharey=False, figsize=(40, 20))\n",
        "    ax = ax.flat\n",
        "    skip = 1\n",
        "\n",
        "    # Validation batch\n",
        "    for ii in np.arange(0, y_true.shape[0], skip):\n",
        "        ax[0].plot(y_true[ii, :, 0], y_true[ii, :, 1], '-')\n",
        "    ax[0].scatter(y_true[:, 0, 0], y_true[:, 0, 1])\n",
        "    ax[0].grid()\n",
        "    ax[0].set_xlabel(\"x1\")\n",
        "    ax[0].set_ylabel(\"x2\")\n",
        "    ax[0].set_title(\"Validation Data (x)\")\n",
        "\n",
        "    # Encoded-advanced-decoded time series\n",
        "    for ii in np.arange(0, enc_adv_dec.shape[0], skip):\n",
        "        ax[1].plot(enc_adv_dec[ii, :, 0], enc_adv_dec[ii, :, 1], '-')\n",
        "    ax[1].scatter(enc_adv_dec[:, 0, 0], enc_adv_dec[:, 0, 1])\n",
        "    ax[1].grid()\n",
        "    ax[1].set_xlabel(\"x1\")\n",
        "    ax[1].set_ylabel(\"x2\")\n",
        "    ax[1].set_title(\"Encoded-Advanced-Decoded (x_adv))\")\n",
        "\n",
        "    # Encoded time series\n",
        "    for ii in np.arange(0, enc.shape[0], skip):\n",
        "        ax[2].plot(enc[ii, :, 0], enc[ii, :, 1], '-')\n",
        "    ax[2].scatter(enc[:, 0, 0], enc[:, 0, 1])\n",
        "    ax[2].grid()\n",
        "    ax[2].set_xlabel(\"y1\")\n",
        "    ax[2].set_ylabel(\"y2\")\n",
        "    ax[2].axis(\"equal\")\n",
        "    ax[2].set_title(\"Encoded (y)\")\n",
        "\n",
        "    # Encoded-decoded time series\n",
        "    for ii in np.arange(0, enc_dec.shape[0], skip):\n",
        "        ax[3].plot(enc_dec[ii, :, 0], enc_dec[ii, :, 1], '-')\n",
        "    ax[3].scatter(enc_dec[:, 0, 0], enc_dec[:, 0, 1])\n",
        "    ax[3].grid()\n",
        "    ax[3].set_xlabel(\"x1\")\n",
        "    ax[3].set_ylabel(\"x2\")\n",
        "    ax[3].set_title(\"Encoded-Decoded (x_ae)\")\n",
        "\n",
        "    # Encoded-advanced time series\n",
        "    for ii in np.arange(0, enc_adv.shape[0], skip):\n",
        "        ax[4].plot(enc_adv[ii, :, 0], enc_adv[ii, :, 1], '-')\n",
        "    ax[4].scatter(enc_adv[:, 0, 0], enc_adv[:, 0, 1])\n",
        "    ax[4].grid()\n",
        "    ax[4].set_xlabel(\"y1\")\n",
        "    ax[4].set_ylabel(\"y2\")\n",
        "    ax[4].axis(\"equal\")\n",
        "    ax[4].set_title(\"Encoded-Advanced (y_adv))\")\n",
        "\n",
        "    # Loss components\n",
        "    lw = 3\n",
        "    loss_comps = np.asarray(loss_comps)\n",
        "    ax[5].plot(val_loss, color='k', linewidth=lw, label='total')\n",
        "    ax[5].set_title(\"Total Loss\")\n",
        "    ax[5].grid()\n",
        "    ax[5].set_xlabel(\"Epoch\")\n",
        "    ax[5].set_ylabel(\"$log_{10}(L)$\")\n",
        "    ax[5].legend(loc=\"upper right\")\n",
        "\n",
        "    ax[6].plot(loss_comps[:, 0], color='r', linewidth=lw, label='recon')\n",
        "    ax[6].set_title(\"Recon Loss\")\n",
        "    ax[6].grid()\n",
        "    ax[6].set_xlabel(\"Epoch\")\n",
        "    ax[6].set_ylabel(\"$log_{10}(L_{recon})$\")\n",
        "    ax[6].legend(loc=\"upper right\")\n",
        "\n",
        "    ax[7].plot(loss_comps[:, 1], color='b', linewidth=lw, label='pred')\n",
        "    ax[7].set_title(\"Prediction Loss\")\n",
        "    ax[7].grid()\n",
        "    ax[7].set_xlabel(\"Epoch\")\n",
        "    ax[7].set_ylabel(\"$log_{10}(L_{pred})$\")\n",
        "    ax[7].legend(loc=\"upper right\")\n",
        "\n",
        "    ax[8].plot(loss_comps[:, 2], color='g', linewidth=lw, label='dmd')\n",
        "    ax[8].set_title(\"DMD\")\n",
        "    ax[8].grid()\n",
        "    ax[8].set_xlabel(\"Epoch\")\n",
        "    ax[8].set_ylabel(\"$log_{10}(L_{dmd})$\")\n",
        "    ax[8].legend(loc=\"upper right\")\n",
        "\n",
        "    fig.suptitle(\n",
        "        \"Epoch: {cur_epoch}/{max_epoch}, Learn Rate: {lr:.5f}, Val. Loss: {loss:.3f}\".format(\n",
        "            cur_epoch=epoch,\n",
        "            max_epoch=hyp_params['max_epochs'],\n",
        "            lr=hyp_params['lr'],\n",
        "            loss=val_loss[-1]))\n",
        "\n",
        "    plt.savefig(save_path)\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def plot_3d(y_pred, y_true, hyp_params, epoch, save_path, loss_comps, val_loss):\n",
        "    enc = y_pred[0]\n",
        "    enc_dec = y_pred[1]\n",
        "    enc_adv_dec = y_pred[2]\n",
        "    enc_adv = y_pred[3]\n",
        "\n",
        "    font = {'family': 'DejaVu Sans', 'size': 10}\n",
        "    matplotlib.rc('font', **font)\n",
        "\n",
        "    skip = 8\n",
        "    fig = plt.figure(figsize=(40, 20))\n",
        "\n",
        "    # Validation batch\n",
        "    ax = fig.add_subplot(3, 3, 1, projection='3d')\n",
        "    for ii in np.arange(0, y_true.shape[0], skip):\n",
        "        ii = int(ii)\n",
        "        x1 = y_true[ii, :, 0]\n",
        "        x2 = y_true[ii, :, 1]\n",
        "        x3 = y_true[ii, :, 2]\n",
        "        ax.plot3D(x1, x2, x3)\n",
        "    ax.set_xlabel(\"$x_{1}$\")\n",
        "    ax.set_ylabel(\"$x_{2}$\")\n",
        "    ax.set_zlabel(\"$x_{3}$\")\n",
        "    ax.set_title(\"Validation Data (x)\")\n",
        "\n",
        "    # Encoded-advanced-decoded time series\n",
        "    ax = fig.add_subplot(3, 3, 2, projection='3d')\n",
        "    for ii in np.arange(0, enc_adv_dec.shape[0], skip):\n",
        "        ii = int(ii)\n",
        "        x1 = enc_adv_dec[ii, :, 0]\n",
        "        x2 = enc_adv_dec[ii, :, 1]\n",
        "        x3 = enc_adv_dec[ii, :, 2]\n",
        "        ax.plot3D(x1, x2, x3)\n",
        "    ax.set_xlabel(\"$x_{1}$\")\n",
        "    ax.set_ylabel(\"$x_{2}$\")\n",
        "    ax.set_zlabel(\"$x_{3}$\")\n",
        "    ax.set_title(\"Encoded-Advanced-Decoded (x_adv))\")\n",
        "\n",
        "    # Encoded time series\n",
        "    ax = fig.add_subplot(3, 3, 3, projection='3d')\n",
        "    for ii in np.arange(0, enc.shape[0], skip):\n",
        "        ii = int(ii)\n",
        "        x1 = enc[ii, :, 0]\n",
        "        x2 = enc[ii, :, 1]\n",
        "        x3 = enc[ii, :, 2]\n",
        "        ax.plot3D(x1, x2, x3)\n",
        "    ax.set_xlabel(\"$y_{1}$\")\n",
        "    ax.set_ylabel(\"$y_{2}$\")\n",
        "    ax.set_zlabel(\"$y_{3}$\")\n",
        "    ax.set_title(\"Encoded (y)\")\n",
        "\n",
        "    # Encoded-decoded time series\n",
        "    ax = fig.add_subplot(3, 3, 4, projection='3d')\n",
        "    for ii in np.arange(0, enc_dec.shape[0], skip):\n",
        "        ii = int(ii)\n",
        "        x1 = enc_dec[ii, :, 0]\n",
        "        x2 = enc_dec[ii, :, 1]\n",
        "        x3 = enc_dec[ii, :, 2]\n",
        "        ax.plot3D(x1, x2, x3)\n",
        "    ax.set_xlabel(\"$x_{1}$\")\n",
        "    ax.set_ylabel(\"$x_{2}$\")\n",
        "    ax.set_zlabel(\"$x_{3}$\")\n",
        "    ax.set_title(\"Encoded-Decoded (x_ae)\")\n",
        "\n",
        "    # Encoded-advanced time series\n",
        "    ax = fig.add_subplot(3, 3, 5, projection='3d')\n",
        "    for ii in np.arange(0, enc_adv.shape[0], skip):\n",
        "        ii = int(ii)\n",
        "        x1 = enc_adv[ii, :, 0]\n",
        "        x2 = enc_adv[ii, :, 1]\n",
        "        x3 = enc_adv[ii, :, 2]\n",
        "        ax.plot3D(x1, x2, x3)\n",
        "    ax.set_xlabel(\"$y_{1}$\")\n",
        "    ax.set_ylabel(\"$y_{2}$\")\n",
        "    ax.set_zlabel(\"$y_{3}$\")\n",
        "    ax.set_title(\"Encoded-Advanced (y_adv))\")\n",
        "\n",
        "    # Loss components\n",
        "    lw = 3\n",
        "    loss_comps = np.asarray(loss_comps)\n",
        "    ax = fig.add_subplot(3, 3, 6)\n",
        "    ax.plot(val_loss, color='k', linewidth=lw, label='total')\n",
        "    ax.set_title(\"Total Loss\")\n",
        "    ax.grid()\n",
        "    ax.set_xlabel(\"Epoch\")\n",
        "    ax.set_ylabel(\"$log_{10}(L)$\")\n",
        "    ax.legend(loc=\"upper right\")\n",
        "\n",
        "    ax = fig.add_subplot(3, 3, 7)\n",
        "    ax.plot(loss_comps[:, 0], color='r', linewidth=lw, label='recon')\n",
        "    ax.set_title(\"Recon Loss\")\n",
        "    ax.grid()\n",
        "    ax.set_xlabel(\"Epoch\")\n",
        "    ax.set_ylabel(\"$log_{10}(L_{recon})$\")\n",
        "    ax.legend(loc=\"upper right\")\n",
        "\n",
        "    ax = fig.add_subplot(3, 3, 8)\n",
        "    ax.plot(loss_comps[:, 1], color='b', linewidth=lw, label='pred')\n",
        "    ax.set_title(\"Prediction Loss\")\n",
        "    ax.grid()\n",
        "    ax.set_xlabel(\"Epoch\")\n",
        "    ax.set_ylabel(\"$log_{10}(L_{pred})$\")\n",
        "    ax.legend(loc=\"upper right\")\n",
        "\n",
        "    ax = fig.add_subplot(3, 3, 9)\n",
        "    ax.plot(loss_comps[:, 2], color='g', linewidth=lw, label='dmd')\n",
        "    ax.set_title(\"DMD\")\n",
        "    ax.grid()\n",
        "    ax.set_xlabel(\"Epoch\")\n",
        "    ax.set_ylabel(\"$log_{10}(L_{dmd})$\")\n",
        "    ax.legend(loc=\"upper right\")\n",
        "\n",
        "    fig.suptitle(\n",
        "        \"Epoch: {cur_epoch}/{max_epoch}, Learn Rate: {lr:.5f}, Val. Loss: {loss:.3f}\".format(\n",
        "            cur_epoch=epoch,\n",
        "            max_epoch=hyp_params['max_epochs'],\n",
        "            lr=hyp_params['lr'],\n",
        "            loss=val_loss[-1]))\n",
        "\n",
        "    plt.savefig(save_path)\n",
        "    plt.close()\n"
      ],
      "metadata": {
        "id": "3bfUhItsW4i5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "    Created by:\n",
        "        Jay Lago - 23 Dec 2020\n",
        "\"\"\"\n",
        "import os\n",
        "import pickle\n",
        "import time\n",
        "import datetime as dt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "def train_model(hyp_params, train_data, val_set, model, loss):\n",
        "    # Dictionary to store all relevant training parameters and losses\n",
        "    train_params = dict()\n",
        "    train_params['start_time'] = time.time()\n",
        "    train_params['train_loss_results'] = []\n",
        "    train_params['val_loss_results'] = []\n",
        "    train_params['val_loss_comps_avgs'] = []\n",
        "    # Step = 1\n",
        "    # total = -16\n",
        "    # total_steps = [total]\n",
        "    # original_window = model.window\n",
        "    for epoch in range(1, hyp_params['max_epochs'] + 1):\n",
        "        epoch_start_time = dt.datetime.now()\n",
        "        epoch_time = time.time()\n",
        "        epoch_loss_avg_train = tf.keras.metrics.Mean()\n",
        "        epoch_loss_avg_val = tf.keras.metrics.Mean()\n",
        "        # epoch_steps = []\n",
        "        # model.window = original_window\n",
        "        # Shuffle, batch, and prefetch training data to the GPU\n",
        "        train_set = train_data.shuffle(hyp_params['num_train_init_conds']) \\\n",
        "            .batch(hyp_params['batch_size'], drop_remainder=True)\n",
        "        train_set = train_set.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "        # Set optimizer\n",
        "        if hyp_params['optimizer'] == 'adam':\n",
        "            myoptimizer = tf.keras.optimizers.Adam(hyp_params['lr'])\n",
        "        if hyp_params['optimizer'] == 'sgd':\n",
        "            myoptimizer = tf.keras.optimizers.SGD(learning_rate=hyp_params['lr'], momentum=0.9)\n",
        "\n",
        "        # Begin batch training\n",
        "        with tf.device(hyp_params['device']):\n",
        "            for train_batch in train_set:\n",
        "                with tf.GradientTape() as tape:\n",
        "                    train_pred = model(train_batch, training=True)\n",
        "                    train_loss = loss(train_pred, train_batch)\n",
        "                gradients = tape.gradient(train_loss, model.trainable_weights)\n",
        "                myoptimizer.apply_gradients([(grad, var) for (grad, var) in zip(gradients, model.trainable_weights)\n",
        "                                             if grad is not None])\n",
        "                myoptimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
        "                epoch_loss_avg_train.update_state(train_loss)\n",
        "\n",
        "            # Batch validation\n",
        "            lrecon = tf.keras.metrics.Mean()\n",
        "            lpred = tf.keras.metrics.Mean()\n",
        "            ldmd = tf.keras.metrics.Mean()\n",
        "\n",
        "            for val_batch in val_set:\n",
        "                val_pred = model(val_batch)\n",
        "                val_loss = loss(val_pred, val_batch)\n",
        "                epoch_loss_avg_val.update_state(val_loss)\n",
        "                # Save loss components for diagnostic plotting\n",
        "                lrecon.update_state(np.log10(loss.loss_recon))\n",
        "                lpred.update_state(np.log10(loss.loss_pred))\n",
        "                ldmd.update_state(np.log10(loss.loss_dmd))\n",
        "                \n",
        "                # if model.window + Step >= model.num_pred_steps:\n",
        "                #     range_list = np.array([-Step, 0])\n",
        "                # elif model.window - Step <= model.num_pred_steps - 80:\n",
        "                #     range_list = np.array([0, Step])\n",
        "                # else:\n",
        "                #     range_list = np.array([-Step, 0, Step])\n",
        "\n",
        "                # val_preds = [None]*np.size(range_list)\n",
        "                # val_losses = np.zeros(np.size(range_list), dtype=np.float64)\n",
        "                # for jj in range(np.size(range_list)):\n",
        "                #     model.window = original_window + range_list[jj]\n",
        "                #     val_preds[jj] = model(val_batch)\n",
        "                #     val_losses[jj] = loss(val_preds[jj], val_batch)\n",
        "\n",
        "                # min_ind = np.argmin(val_losses)\n",
        "                # val_pred = val_preds[min_ind]\n",
        "                # val_loss = val_losses[min_ind]\n",
        "                # epoch_steps.append(range_list[min_ind])               \n",
        "\n",
        "                # epoch_loss_avg_val.update_state(val_loss)\n",
        "                # # Save loss components for diagnostic plotting\n",
        "                # lrecon.update_state(np.log10(loss.loss_recon))\n",
        "                # lpred.update_state(np.log10(loss.loss_pred))\n",
        "                # ldmd.update_state(np.log10(loss.loss_dmd))\n",
        "\n",
        "            train_params['val_loss_comps_avgs'].append([lrecon.result(), lpred.result(), ldmd.result()])\n",
        "\n",
        "        # Report training status\n",
        "        train_params['train_loss_results'].append(np.log10(epoch_loss_avg_train.result()))\n",
        "        train_params['val_loss_results'].append(np.log10(epoch_loss_avg_val.result()))\n",
        "        print(\"Epoch {epoch} of {max_epoch} / Train {train:3.7f} / Val {test:3.7f} / LR {lr:2.7f} / {time:4.2f} seconds\"\n",
        "              .format(epoch=epoch, max_epoch=hyp_params['max_epochs'],\n",
        "                      train=train_params['train_loss_results'][-1],\n",
        "                      test=train_params['val_loss_results'][-1],\n",
        "                      lr=hyp_params['lr'],\n",
        "                      time=time.time() - epoch_time))\n",
        "\n",
        "        # mode = max(set(epoch_steps), key=epoch_steps.count)\n",
        "        # print(\"The most common window shift was: \", mode)\n",
        "        # original_window = original_window + mode\n",
        "        # print(\"New window size for the next epoch: \", original_window)\n",
        "        # total = total + mode\n",
        "        # total_steps.append(total)        \n",
        "        \n",
        "        # Save training diagnostic plots\n",
        "        if epoch == 1 or epoch % hyp_params['plot_every'] == 0:\n",
        "            if not os.path.exists(hyp_params['plot_path']):\n",
        "                os.makedirs(hyp_params['plot_path'])\n",
        "            this_plot = hyp_params['plot_path'] + '/' + epoch_start_time.strftime(\"%Y%m%d%H%M%S\") + '.png'\n",
        "            diagnostic_plot(val_pred, val_batch, hyp_params, epoch,\n",
        "                               this_plot, train_params['val_loss_comps_avgs'],\n",
        "                               train_params['val_loss_results'])\n",
        "\n",
        "        # Save model\n",
        "        if epoch % hyp_params['save_every'] == 0 or epoch == hyp_params['max_epochs']:\n",
        "            if not os.path.exists(hyp_params['model_path']):\n",
        "                os.makedirs(hyp_params['model_path'])\n",
        "            model_path = hyp_params['model_path'] + '/epoch_{epoch}_loss_{loss:2.3}' \\\n",
        "                .format(epoch=epoch, loss=train_params['val_loss_results'][-1])\n",
        "            model.save_weights(model_path + '.h5')\n",
        "            pickle.dump(hyp_params, open(model_path + '.pkl', 'wb'))\n",
        "\n",
        "    print(\"\\nTotal training time: %4.2f minutes\" % ((time.time() - train_params['start_time']) / 60.0))\n",
        "    print(\"Final train loss: %2.7f\" % (train_params['train_loss_results'][-1]))\n",
        "    print(\"Final validation loss: %2.7f\" % (train_params['val_loss_results'][-1]))\n",
        "    # print(\"here are the delay steps taken: \")\n",
        "    # print(total_steps)\n",
        "    # hf.net_steps_plot(total_steps)\n",
        "    results = dict()\n",
        "    results['model'] = model\n",
        "    results['loss'] = loss\n",
        "    results['val_loss_history'] = train_params['val_loss_results']\n",
        "    results['val_loss_comps'] = train_params['val_loss_comps_avgs']\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "rrC_jeoXW-jI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "    Author:\n",
        "        Jay Lago, NIWC/SDSU, 2021\n",
        "\"\"\"\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import *\n",
        "\n",
        "\n",
        "class HDMD(keras.Model):\n",
        "    def __init__(self, hyp_params, **kwargs):\n",
        "        super(HDMD, self).__init__(**kwargs)\n",
        "\n",
        "        # Parameters\n",
        "        self.batch_size = hyp_params['batch_size']\n",
        "        self.phys_dim = hyp_params['phys_dim']\n",
        "        self.latent_dim = hyp_params['latent_dim']\n",
        "        self.num_time_steps = int(hyp_params['num_time_steps'])\n",
        "        self.num_pred_steps = int(hyp_params['num_pred_steps'])\n",
        "        self.time_final = hyp_params['time_final']\n",
        "        self.num_en_layers = hyp_params['num_en_layers']\n",
        "        self.num_neurons = hyp_params['num_en_neurons']\n",
        "        self.delta_t = hyp_params['delta_t']\n",
        "        self.precision = hyp_params['precision']\n",
        "\n",
        "        self.num_observables = hyp_params['num_observables']\n",
        "        self.threshold = hyp_params['threshold']\n",
        "        self.observation_dimension = hyp_params['observation_dimension']\n",
        "        self.window = self.num_time_steps - (self.num_observables - 1)\n",
        "\n",
        "        self.enc_input = (self.num_time_steps, self.phys_dim)\n",
        "        self.dec_input = (self.window-1, self.latent_dim)\n",
        "\n",
        "        if self.precision == 'float32':\n",
        "            self.precision_complex = tf.complex64\n",
        "        else:\n",
        "            self.precision_complex = tf.complex128\n",
        "\n",
        "        # Construct the ENCODER network\n",
        "        self.encoder = keras.Sequential(name=\"encoder\")\n",
        "        self.encoder.add(Dense(self.num_neurons,\n",
        "                               input_shape=self.enc_input,\n",
        "                               activation=hyp_params['hidden_activation'],\n",
        "                               kernel_initializer=hyp_params['kernel_init_enc'],\n",
        "                               bias_initializer=hyp_params['bias_initializer'],\n",
        "                               trainable=True, name='enc_in'))\n",
        "        for ii in range(self.num_en_layers):\n",
        "            self.encoder.add(Dense(self.num_neurons,\n",
        "                                   activation=hyp_params['hidden_activation'],\n",
        "                                   kernel_initializer=hyp_params['kernel_init_enc'],\n",
        "                                   bias_initializer=hyp_params['bias_initializer'],\n",
        "                                   trainable=True, name='enc_' + str(ii)))\n",
        "        self.encoder.add(Dense(self.latent_dim,\n",
        "                               activation=hyp_params['ae_output_activation'],\n",
        "                               kernel_initializer=hyp_params['kernel_init_enc'],\n",
        "                               bias_initializer=hyp_params['bias_initializer'],\n",
        "                               trainable=True, name='enc_out'))\n",
        "\n",
        "        # Construct the DECODER network\n",
        "        self.decoder = keras.Sequential(name=\"decoder\")\n",
        "        self.decoder.add(Dense(self.num_neurons,\n",
        "                               input_shape=self.dec_input,\n",
        "                               activation=hyp_params['hidden_activation'],\n",
        "                               kernel_initializer=hyp_params['kernel_init_enc'],\n",
        "                               bias_initializer=hyp_params['bias_initializer'],\n",
        "                               trainable=True, name='dec_in'))\n",
        "        for ii in range(self.num_en_layers):\n",
        "            self.decoder.add(Dense(self.num_neurons,\n",
        "                                   activation=hyp_params['hidden_activation'],\n",
        "                                   kernel_initializer=hyp_params['kernel_init_dec'],\n",
        "                                   bias_initializer=hyp_params['bias_initializer'],\n",
        "                                   trainable=True, name='dec_' + str(ii)))\n",
        "        self.decoder.add(Dense(self.phys_dim,\n",
        "                               activation=hyp_params['ae_output_activation'],\n",
        "                               kernel_initializer=hyp_params['kernel_init_dec'],\n",
        "                               bias_initializer=hyp_params['bias_initializer'],\n",
        "                               trainable=True, name='dec_out'))\n",
        "\n",
        "    def call(self, x):\n",
        "        # Encode the entire time series\n",
        "        y = self.encoder(x)\n",
        "        x_ae = self.decoder(y[:, :(self.window-1), :])\n",
        "\n",
        "        # Reshape for DMD step\n",
        "        yt = tf.transpose(y, [0, 2, 1])\n",
        "\n",
        "        y_adv, evals, evecs, phi = self.hankel_dmd(yt)\n",
        "\n",
        "        # Generate latent time series using DMD prediction\n",
        "        #y_adv, evals, evecs, phi = self.edmd(yt)\n",
        "\n",
        "        # Decode the latent trajectories\n",
        "        x_adv = self.decoder(y_adv)\n",
        "\n",
        "        # Model weights\n",
        "        weights = self.trainable_weights\n",
        "\n",
        "        return [y, x_ae, x_adv, y_adv, weights, evals, evecs, phi]\n",
        "\n",
        "    def edmd(self, Y):\n",
        "        Y_m = Y[:, :, :-1]\n",
        "        Y_p = Y[:, :, 1:]\n",
        "\n",
        "        sig, U, V = tf.linalg.svd(Y_m, compute_uv=True, full_matrices=False)\n",
        "        sigr_inv = tf.linalg.diag(1.0 / sig)\n",
        "        Uh = tf.linalg.adjoint(U)\n",
        "\n",
        "        A = Y_p @ V @ sigr_inv @ Uh\n",
        "        evals, evecs = tf.linalg.eig(A)\n",
        "        phi = tf.linalg.solve(evecs, tf.cast(Y_m, dtype=self.precision_complex))\n",
        "        y0 = phi[:, :, 0]\n",
        "        y0 = y0[:, :, tf.newaxis]\n",
        "\n",
        "        recon = tf.TensorArray(self.precision_complex, size=self.num_pred_steps)\n",
        "        recon = recon.write(0, evecs @ y0)\n",
        "        evals_k = tf.identity(evals)\n",
        "        for ii in tf.range(1, self.num_pred_steps):\n",
        "            tmp = evecs @ (tf.linalg.diag(evals_k) @ y0)\n",
        "            recon = recon.write(ii, tmp)\n",
        "            evals_k = evals_k * evals\n",
        "        recon = tf.math.real(tf.transpose(tf.squeeze(recon.stack()), perm=[1, 0, 2]))\n",
        "        return recon, evals, evecs, phi\n",
        "\n",
        "    def hankel_matrix(self, tseries):\n",
        "        # note, we are working exclusively in NumPy/SciPy here\n",
        "        tcol = tseries[:self.num_observables]\n",
        "        trow = tseries[(self.num_observables-1):]\n",
        "        hmat = np.flipud(sp.linalg.toeplitz(tcol[::-1], trow))\n",
        "        return hmat\n",
        "\n",
        "    def hankel_dmd(self, Y):\n",
        "        winsize = self.window\n",
        "        nobs = self.num_observables\n",
        "        # Perform DMD method.  Note, we need to be careful about how we break the concantenated Hankel matrix apart.\n",
        "\n",
        "        gm = tf.Variable(tf.zeros([self.num_observables, self.batch_size * (self.window - 1)], dtype=self.precision))\n",
        "        gp = tf.Variable(tf.zeros([self.num_observables, self.batch_size * (self.window - 1)], dtype=self.precision))\n",
        "        Yobserved = (tf.squeeze(Y[:, self.observation_dimension, :])).numpy()\n",
        "\n",
        "        for ll in range(self.batch_size):\n",
        "            tseries = Yobserved[ll, :]\n",
        "            tcol = tseries[:nobs]\n",
        "            trow = tseries[(nobs - 1):]\n",
        "            hmat = np.flipud(sp.linalg.toeplitz(tcol[::-1], trow))\n",
        "            gm[:, ll * (winsize - 1):(ll + 1) * (winsize - 1)].assign(hmat[:, :-1])\n",
        "            gp[:, ll * (winsize - 1):(ll + 1) * (winsize - 1)].assign(hmat[:, 1:])\n",
        "\n",
        "        sig, U, V = tf.linalg.svd(gm, compute_uv=True, full_matrices=False)\n",
        "        sig_inv = tf.linalg.diag(1.0 / sig)\n",
        "        Uh = tf.linalg.adjoint(U)\n",
        "        A = gp @ V @ sig_inv @ Uh\n",
        "        evals, evecs = tf.linalg.eig(A)\n",
        "        phi = tf.linalg.solve(evecs, tf.cast(gm, dtype=self.precision_complex))\n",
        "\n",
        "        # Build reconstruction\n",
        "        phiinit = phi[:, ::(self.window-1)]\n",
        "        initconds = tf.cast(tf.transpose(tf.squeeze(Y[:, :, 0])), dtype=self.precision_complex)\n",
        "        sigp, Up, Vp = tf.linalg.svd(phiinit, compute_uv=True, full_matrices=False)\n",
        "        sigp_inv = tf.cast(tf.linalg.diag(1.0 / sigp), dtype=self.precision_complex)\n",
        "        kmat = initconds @ Vp @ sigp_inv @ tf.linalg.adjoint(Up)\n",
        "        recon = tf.reshape(tf.transpose(tf.math.real(kmat @ phi)), [self.batch_size, self.window-1, self.phys_dim])\n",
        "        return recon, evals, evecs, phi\n",
        "\n",
        "    def get_config(self):\n",
        "        base_config = super().get_config()\n",
        "        return {**base_config,\n",
        "                'encoder': self.encoder,\n",
        "                'decoder': self.decoder}\n"
      ],
      "metadata": {
        "id": "IdrPZrp8W_bP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "    Author:\n",
        "        Jay Lago, NIWC/SDSU, 2021\n",
        "\"\"\"\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.losses import MSE\n",
        "\n",
        "\n",
        "class LossDLDMD(keras.losses.Loss):\n",
        "    def __init__(self, hyp_params, **kwargs):\n",
        "        super(LossDLDMD, self).__init__(**kwargs)\n",
        "\n",
        "        # Parameters\n",
        "        self.a1 = hyp_params['a1']\n",
        "        self.a2 = hyp_params['a2']\n",
        "        self.a3 = hyp_params['a3']\n",
        "        self.a4 = hyp_params['a4']\n",
        "        self.precision = hyp_params['precision']\n",
        "        self.num_time_steps = int(hyp_params['num_time_steps'])\n",
        "        self.num_observables = hyp_params['num_observables']\n",
        "        self.window = self.num_time_steps - (self.num_observables - 1)\n",
        "\n",
        "        # Loss components\n",
        "        self.loss_recon = tf.constant(0.0, dtype=self.precision)\n",
        "        self.loss_pred = tf.constant(0.0, dtype=self.precision)\n",
        "        self.loss_dmd = tf.constant(0.0, dtype=self.precision)\n",
        "        self.loss_reg = tf.constant(0.0, dtype=self.precision)\n",
        "        self.total_loss = tf.constant(0.0, dtype=self.precision)\n",
        "\n",
        "    def call(self, model, obs):\n",
        "        \"\"\"\n",
        "            model = [y, x_ae, x_adv, y_adv_real, weights, evals, evecs, phi]\n",
        "        \"\"\"\n",
        "        y = tf.identity(model[0])\n",
        "        x_ae = tf.identity(model[1])\n",
        "        x_adv = tf.identity(model[2])\n",
        "        weights = model[4]\n",
        "        pred_horizon = -1\n",
        "        obs_windowed = obs[:, :(self.window-1), :]\n",
        "        # Autoencoder reconstruction\n",
        "        self.loss_recon = tf.reduce_mean(MSE(obs_windowed, x_ae))\n",
        "\n",
        "        # DMD reconstruction in the latent space\n",
        "        self.loss_dmd = self.dmdloss(y)\n",
        "\n",
        "        # Future state prediction\n",
        "        self.loss_pred = tf.reduce_mean(MSE(obs_windowed[:, :pred_horizon, :], x_adv[:, :pred_horizon, :]))\n",
        "\n",
        "        # Regularization on weights\n",
        "        self.loss_reg = tf.add_n([tf.nn.l2_loss(w) for w in weights])\n",
        "\n",
        "        # Total loss\n",
        "        self.total_loss = self.a1 * self.loss_recon + self.a2 * self.loss_dmd + \\\n",
        "                          self.a3 * self.loss_pred + self.a4 * self.loss_reg\n",
        "\n",
        "        return self.total_loss\n",
        "\n",
        "    @tf.function\n",
        "    def dmdloss(self, y):\n",
        "        y_m = tf.transpose(y, perm=[0, 2, 1])[:, :, :-1]\n",
        "        y_p = tf.transpose(y, perm=[0, 2, 1])[:, :, 1:]\n",
        "        [_, _, V] = tf.linalg.svd(y_m, compute_uv=True, full_matrices=False)\n",
        "        VVh = V @ tf.linalg.adjoint(V)\n",
        "        eye_mat = tf.eye(VVh.shape[-1], batch_shape=[VVh.shape[0]], dtype=self.precision)\n",
        "        return tf.reduce_mean(tf.norm(y_p @ (eye_mat - VVh), ord='fro', axis=[-2, -1]))\n",
        "\n",
        "    def get_config(self):\n",
        "        base_config = super().get_config()\n",
        "        return {**base_config,\n",
        "                'loss_recon': self.loss_recon,\n",
        "                'loss_pred': self.loss_pred,\n",
        "                'loss_dmd': self.loss_dmd,\n",
        "                'loss_inf': self.loss_inf,\n",
        "                'loss_reg': self.loss_reg,\n",
        "                'total_loss': self.total_loss}\n"
      ],
      "metadata": {
        "id": "fguOupqdceeE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "    Author:\n",
        "        Jay Lago, NIWC/SDSU, 2021\n",
        "\"\"\"\n",
        "import tensorflow as tf\n",
        "import pickle\n",
        "import datetime as dt\n",
        "import os\n",
        "import sys\n",
        "sys.path.insert(0, '../../')\n",
        "# import HDMD as dl\n",
        "# import LossDLDMD as lf\n",
        "# import Data as dat\n",
        "# import Training as tr\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# Setup\n",
        "# ==============================================================================\n",
        "NUM_SAVES = 1       # Number of times to save the model throughout training\n",
        "NUM_PLOTS = 100      # Number of diagnostic plots to generate while training\n",
        "DEVICE = '/GPU:0'\n",
        "GPUS = tf.config.experimental.list_physical_devices('GPU')\n",
        "if GPUS:\n",
        "    try:\n",
        "        for gpu in GPUS:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "    except RuntimeError as e:\n",
        "        print(e)\n",
        "else:\n",
        "    DEVICE = '/CPU:0'\n",
        "\n",
        "tf.keras.backend.set_floatx('float64')  # !! Set precision for the entire model here\n",
        "print(\"TensorFlow version: {}\".format(tf.__version__))\n",
        "print(\"Eager execution: {}\".format(tf.executing_eagerly()))\n",
        "print(\"Num GPUs available: {}\".format(len(GPUS)))\n",
        "print(\"Training at precision: {}\".format(tf.keras.backend.floatx()))\n",
        "print(\"Training on device: {}\".format(DEVICE))\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# Initialize hyper-parameters and model\n",
        "# ==============================================================================\n",
        "# General parameters\n",
        "hyp_params = dict()\n",
        "hyp_params['sim_start'] = dt.datetime.now().strftime(\"%Y-%m-%d-%H%M\")\n",
        "hyp_params['experiment'] = 'pendulum'\n",
        "hyp_params['plot_path'] = './training_results/' + hyp_params['experiment'] + '_' + hyp_params['sim_start']\n",
        "hyp_params['model_path'] = './trained_models/' + hyp_params['experiment'] + '_' + hyp_params['sim_start']\n",
        "hyp_params['device'] = DEVICE\n",
        "hyp_params['precision'] = tf.keras.backend.floatx()\n",
        "hyp_params['num_init_conds'] = 15000\n",
        "hyp_params['num_train_init_conds'] = 10000\n",
        "hyp_params['num_val_init_conds'] = 3000\n",
        "hyp_params['num_test_init_conds'] = 2000\n",
        "hyp_params['time_final'] = 6\n",
        "hyp_params['delta_t'] = 0.02\n",
        "hyp_params['num_time_steps'] = int(hyp_params['time_final']/hyp_params['delta_t'] + 1)\n",
        "hyp_params['num_pred_steps'] = hyp_params['num_time_steps']\n",
        "hyp_params['max_epochs'] = 100\n",
        "hyp_params['save_every'] = hyp_params['max_epochs'] // NUM_SAVES\n",
        "hyp_params['plot_every'] = hyp_params['max_epochs'] // NUM_PLOTS\n",
        "\n",
        "# Hankel-DMD Window Size and Threshhold\n",
        "hyp_params['num_observables'] = 40\n",
        "hyp_params['threshold'] = 6\n",
        "hyp_params['observation_dimension'] = 0\n",
        "\n",
        "# Universal network layer parameters (AE & Aux)\n",
        "hyp_params['optimizer'] = 'adam'\n",
        "hyp_params['batch_size'] = 128\n",
        "hyp_params['phys_dim'] = 2\n",
        "hyp_params['latent_dim'] = 2\n",
        "hyp_params['hidden_activation'] = tf.keras.activations.relu\n",
        "hyp_params['bias_initializer'] = tf.keras.initializers.zeros\n",
        "\n",
        "# Encoding/Decoding Layer Parameters\n",
        "hyp_params['num_en_layers'] = 3\n",
        "hyp_params['num_en_neurons'] = 128\n",
        "hyp_params['kernel_init_enc'] = tf.keras.initializers.TruncatedNormal(mean=0.0, stddev=0.1)\n",
        "hyp_params['kernel_init_dec'] = tf.keras.initializers.TruncatedNormal(mean=0.0, stddev=0.1)\n",
        "hyp_params['ae_output_activation'] = tf.keras.activations.linear\n",
        "\n",
        "# Loss Function Parameters\n",
        "hyp_params['a1'] = tf.constant(1, dtype=hyp_params['precision'])        # Reconstruction\n",
        "hyp_params['a2'] = tf.constant(1, dtype=hyp_params['precision'])        # DMD\n",
        "hyp_params['a3'] = tf.constant(1, dtype=hyp_params['precision'])        # Prediction\n",
        "hyp_params['a4'] = tf.constant(1e-14, dtype=hyp_params['precision'])    # L-2 on weights\n",
        "\n",
        "# Learning rate\n",
        "hyp_params['lr'] = 1e-3\n",
        "\n",
        "# Initialize the Koopman model and loss\n",
        "myMachine = HDMD(hyp_params)\n",
        "myLoss = LossDLDMD(hyp_params)\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# Generate / load data\n",
        "# ==============================================================================\n",
        "data_fname = 'pendulum_data.pkl'\n",
        "if os.path.exists(data_fname):\n",
        "    # Load data from file\n",
        "    data = pickle.load(open(data_fname, 'rb'))\n",
        "    data = tf.cast(data, dtype=hyp_params['precision'])\n",
        "else:\n",
        "    # Create new data\n",
        "    data = data_maker_pendulum(x_lower1=-3.1, x_upper1=3.1, x_lower2=-2, x_upper2=2,\n",
        "                                   n_ic=hyp_params['num_init_conds'], dt=hyp_params['delta_t'],\n",
        "                                   tf=hyp_params['time_final'])\n",
        "    data = tf.cast(data, dtype=hyp_params['precision'])\n",
        "    # Save data to file\n",
        "    pickle.dump(data, open(data_fname, 'wb'))\n",
        "\n",
        "# Create training and validation datasets from the initial conditions\n",
        "shuffled_data = tf.random.shuffle(data)\n",
        "ntic = hyp_params['num_train_init_conds']\n",
        "nvic = hyp_params['num_val_init_conds']\n",
        "train_data = shuffled_data[:ntic, :, :]\n",
        "val_data = shuffled_data[ntic:ntic+nvic, :, :]\n",
        "test_data = shuffled_data[ntic+nvic:, :, :]\n",
        "pickle.dump(train_data, open('data_train.pkl', 'wb'))\n",
        "pickle.dump(val_data, open('data_val.pkl', 'wb'))\n",
        "pickle.dump(test_data, open('data_test.pkl', 'wb'))\n",
        "train_data = tf.data.Dataset.from_tensor_slices(train_data)\n",
        "val_data = tf.data.Dataset.from_tensor_slices(val_data)\n",
        "test_data = tf.data.Dataset.from_tensor_slices(test_data)\n",
        "\n",
        "# Batch and prefetch the validation data to the GPUs\n",
        "val_set = val_data.batch(hyp_params['batch_size'], drop_remainder=True)\n",
        "#val_set = val_set.prefetch(tf.data.AUTOTUNE)\n",
        "try:\n",
        "    val_set = val_set.prefetch(tf.data.AUTOTUNE)\n",
        "except:\n",
        "    val_set = val_set.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "# ==============================================================================\n",
        "# Train the model\n",
        "# ==============================================================================\n",
        "results = train_model(hyp_params=hyp_params, train_data=train_data, val_set=val_set, model=myMachine, loss=myLoss)\n",
        "print(results['model'].summary())\n",
        "exit()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWSoHQTmXC9q",
        "outputId": "271e7a61-11cc-4c67-d9ec-0aade0684d6a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.7.0\n",
            "Eager execution: True\n",
            "Num GPUs available: 1\n",
            "Training at precision: float64\n",
            "Training on device: /GPU:0\n",
            "Epoch 1 of 100 / Train -0.1082752 / Val -0.9517576 / LR 0.0010000 / 89.41 seconds\n",
            "Epoch 2 of 100 / Train -1.1055799 / Val -0.9712081 / LR 0.0010000 / 84.79 seconds\n",
            "Epoch 3 of 100 / Train -1.2007374 / Val -1.2403337 / LR 0.0010000 / 83.83 seconds\n",
            "Epoch 4 of 100 / Train -1.2442844 / Val -1.2721107 / LR 0.0010000 / 84.47 seconds\n",
            "Epoch 5 of 100 / Train -1.2758977 / Val -1.4579672 / LR 0.0010000 / 91.20 seconds\n",
            "Epoch 6 of 100 / Train -1.3819806 / Val -1.7292736 / LR 0.0010000 / 85.01 seconds\n",
            "Epoch 7 of 100 / Train -1.3848215 / Val -1.7753781 / LR 0.0010000 / 85.23 seconds\n",
            "Epoch 8 of 100 / Train -1.3302979 / Val -1.5946782 / LR 0.0010000 / 84.83 seconds\n",
            "Epoch 9 of 100 / Train -1.3521401 / Val -1.9119658 / LR 0.0010000 / 84.67 seconds\n",
            "Epoch 10 of 100 / Train -1.3753037 / Val -1.7012292 / LR 0.0010000 / 84.85 seconds\n",
            "Epoch 11 of 100 / Train -1.4184027 / Val -1.4380908 / LR 0.0010000 / 84.30 seconds\n",
            "Epoch 12 of 100 / Train -1.4226536 / Val -1.1758515 / LR 0.0010000 / 84.62 seconds\n",
            "Epoch 13 of 100 / Train -1.4459510 / Val -1.4738572 / LR 0.0010000 / 84.25 seconds\n",
            "Epoch 14 of 100 / Train -1.5465388 / Val -1.4886944 / LR 0.0010000 / 84.94 seconds\n",
            "Epoch 15 of 100 / Train -1.5041850 / Val -1.4543327 / LR 0.0010000 / 84.28 seconds\n",
            "Epoch 16 of 100 / Train -1.4760064 / Val -1.7503842 / LR 0.0010000 / 85.19 seconds\n",
            "Epoch 17 of 100 / Train -1.4716945 / Val -1.4129566 / LR 0.0010000 / 84.10 seconds\n",
            "Epoch 18 of 100 / Train -1.5506529 / Val -1.2252798 / LR 0.0010000 / 83.55 seconds\n",
            "Epoch 19 of 100 / Train -1.5880698 / Val -1.4913936 / LR 0.0010000 / 84.80 seconds\n",
            "Epoch 20 of 100 / Train -1.5588974 / Val -1.6218690 / LR 0.0010000 / 84.44 seconds\n",
            "Epoch 21 of 100 / Train -1.5359274 / Val -1.5644069 / LR 0.0010000 / 85.41 seconds\n",
            "Epoch 22 of 100 / Train -1.6033959 / Val -1.6373527 / LR 0.0010000 / 84.61 seconds\n",
            "Epoch 23 of 100 / Train -1.6573980 / Val -1.6533088 / LR 0.0010000 / 84.37 seconds\n",
            "Epoch 24 of 100 / Train -1.5616255 / Val -1.6265128 / LR 0.0010000 / 84.67 seconds\n",
            "Epoch 25 of 100 / Train -1.6226772 / Val -1.5799970 / LR 0.0010000 / 84.77 seconds\n",
            "Epoch 26 of 100 / Train -1.5900571 / Val -1.6563875 / LR 0.0010000 / 84.77 seconds\n",
            "Epoch 27 of 100 / Train -1.6017968 / Val -1.5866811 / LR 0.0010000 / 84.73 seconds\n",
            "Epoch 28 of 100 / Train -1.6188814 / Val -1.5279178 / LR 0.0010000 / 84.35 seconds\n",
            "Epoch 29 of 100 / Train -1.6699230 / Val -1.8494985 / LR 0.0010000 / 84.60 seconds\n",
            "Epoch 30 of 100 / Train -1.5761762 / Val -1.4645142 / LR 0.0010000 / 85.18 seconds\n",
            "Epoch 31 of 100 / Train -1.6570373 / Val -1.3379610 / LR 0.0010000 / 84.74 seconds\n",
            "Epoch 32 of 100 / Train -1.6021894 / Val -1.8719661 / LR 0.0010000 / 85.18 seconds\n",
            "Epoch 33 of 100 / Train -1.6587115 / Val -1.5384753 / LR 0.0010000 / 85.07 seconds\n",
            "Epoch 34 of 100 / Train -1.6956707 / Val -1.8901279 / LR 0.0010000 / 85.14 seconds\n",
            "Epoch 35 of 100 / Train -1.6502453 / Val -1.7936796 / LR 0.0010000 / 85.52 seconds\n",
            "Epoch 36 of 100 / Train -1.7023898 / Val -1.5693497 / LR 0.0010000 / 85.45 seconds\n",
            "Epoch 37 of 100 / Train -1.6548285 / Val -1.6110320 / LR 0.0010000 / 84.89 seconds\n",
            "Epoch 38 of 100 / Train -1.6852872 / Val -1.7904936 / LR 0.0010000 / 85.17 seconds\n",
            "Epoch 39 of 100 / Train -1.7466818 / Val -1.8010938 / LR 0.0010000 / 85.28 seconds\n",
            "Epoch 40 of 100 / Train -1.7049494 / Val -1.7496737 / LR 0.0010000 / 84.66 seconds\n",
            "Epoch 41 of 100 / Train -1.6485530 / Val -1.8051592 / LR 0.0010000 / 84.75 seconds\n",
            "Epoch 42 of 100 / Train -1.6632411 / Val -1.5295240 / LR 0.0010000 / 85.05 seconds\n",
            "Epoch 43 of 100 / Train -1.6164875 / Val -1.4413000 / LR 0.0010000 / 85.29 seconds\n",
            "Epoch 44 of 100 / Train -1.6872937 / Val -1.7189341 / LR 0.0010000 / 84.98 seconds\n",
            "Epoch 45 of 100 / Train -1.6921078 / Val -2.2264688 / LR 0.0010000 / 85.21 seconds\n",
            "Epoch 46 of 100 / Train -1.7923277 / Val -1.8217314 / LR 0.0010000 / 85.04 seconds\n",
            "Epoch 47 of 100 / Train -1.6691858 / Val -1.7136512 / LR 0.0010000 / 85.23 seconds\n",
            "Epoch 48 of 100 / Train -1.7816501 / Val -1.7298822 / LR 0.0010000 / 85.53 seconds\n",
            "Epoch 49 of 100 / Train -1.7758562 / Val -1.6729918 / LR 0.0010000 / 85.25 seconds\n",
            "Epoch 50 of 100 / Train -1.7154980 / Val -1.9589300 / LR 0.0010000 / 84.74 seconds\n",
            "Epoch 51 of 100 / Train -1.7672034 / Val -1.8329731 / LR 0.0010000 / 85.21 seconds\n",
            "Epoch 52 of 100 / Train -1.7585865 / Val -1.9707431 / LR 0.0010000 / 85.17 seconds\n",
            "Epoch 53 of 100 / Train -1.7287973 / Val -1.5556246 / LR 0.0010000 / 87.22 seconds\n",
            "Epoch 54 of 100 / Train -1.7605628 / Val -1.6338898 / LR 0.0010000 / 85.20 seconds\n",
            "Epoch 55 of 100 / Train -1.7546566 / Val -1.8548923 / LR 0.0010000 / 85.34 seconds\n",
            "Epoch 56 of 100 / Train -1.7384583 / Val -2.0442761 / LR 0.0010000 / 84.35 seconds\n",
            "Epoch 57 of 100 / Train -1.7409114 / Val -1.9761890 / LR 0.0010000 / 85.32 seconds\n",
            "Epoch 58 of 100 / Train -1.8299095 / Val -1.9148762 / LR 0.0010000 / 85.18 seconds\n",
            "Epoch 59 of 100 / Train -1.7676987 / Val -1.7549455 / LR 0.0010000 / 85.40 seconds\n",
            "Epoch 60 of 100 / Train -1.8243038 / Val -1.9798080 / LR 0.0010000 / 84.92 seconds\n",
            "Epoch 61 of 100 / Train -1.8331759 / Val -1.7337458 / LR 0.0010000 / 84.76 seconds\n",
            "Epoch 62 of 100 / Train -1.8048000 / Val -1.9000004 / LR 0.0010000 / 84.96 seconds\n",
            "Epoch 63 of 100 / Train -1.8195921 / Val -1.6010054 / LR 0.0010000 / 85.65 seconds\n",
            "Epoch 64 of 100 / Train -1.7312264 / Val -1.9624618 / LR 0.0010000 / 88.46 seconds\n",
            "Epoch 65 of 100 / Train -1.7648719 / Val -1.5931650 / LR 0.0010000 / 86.32 seconds\n",
            "Epoch 66 of 100 / Train -1.8073638 / Val -1.6571412 / LR 0.0010000 / 86.11 seconds\n",
            "Epoch 67 of 100 / Train -1.8191881 / Val -1.8491901 / LR 0.0010000 / 86.29 seconds\n",
            "Epoch 68 of 100 / Train -1.8167930 / Val -1.8019229 / LR 0.0010000 / 92.91 seconds\n",
            "Epoch 69 of 100 / Train -1.8488999 / Val -1.9268082 / LR 0.0010000 / 86.66 seconds\n",
            "Epoch 70 of 100 / Train -1.8214970 / Val -1.9298978 / LR 0.0010000 / 86.26 seconds\n",
            "Epoch 71 of 100 / Train -1.8322347 / Val -1.8870203 / LR 0.0010000 / 86.32 seconds\n",
            "Epoch 72 of 100 / Train -1.8041546 / Val -1.7173665 / LR 0.0010000 / 86.55 seconds\n",
            "Epoch 73 of 100 / Train -1.7428460 / Val -2.2198531 / LR 0.0010000 / 86.04 seconds\n",
            "Epoch 74 of 100 / Train -1.8256950 / Val -1.6047177 / LR 0.0010000 / 86.43 seconds\n",
            "Epoch 75 of 100 / Train -1.7905290 / Val -2.1654773 / LR 0.0010000 / 86.16 seconds\n",
            "Epoch 76 of 100 / Train -1.9194143 / Val -1.7633371 / LR 0.0010000 / 85.53 seconds\n",
            "Epoch 77 of 100 / Train -1.8341049 / Val -1.7933062 / LR 0.0010000 / 88.18 seconds\n",
            "Epoch 78 of 100 / Train -1.8790606 / Val -1.9446862 / LR 0.0010000 / 85.67 seconds\n",
            "Epoch 79 of 100 / Train -1.8884105 / Val -2.3241266 / LR 0.0010000 / 85.60 seconds\n",
            "Epoch 80 of 100 / Train -1.8509987 / Val -1.7968730 / LR 0.0010000 / 85.83 seconds\n",
            "Epoch 81 of 100 / Train -1.8462463 / Val -1.8675217 / LR 0.0010000 / 86.27 seconds\n",
            "Epoch 82 of 100 / Train -1.9284010 / Val -1.9959957 / LR 0.0010000 / 85.76 seconds\n",
            "Epoch 83 of 100 / Train -1.8503439 / Val -2.2636377 / LR 0.0010000 / 85.76 seconds\n",
            "Epoch 84 of 100 / Train -1.8633115 / Val -1.9931984 / LR 0.0010000 / 85.64 seconds\n",
            "Epoch 85 of 100 / Train -1.9380133 / Val -1.8010673 / LR 0.0010000 / 85.45 seconds\n",
            "Epoch 86 of 100 / Train -1.8597537 / Val -1.7172662 / LR 0.0010000 / 86.01 seconds\n",
            "Epoch 87 of 100 / Train -1.8553439 / Val -1.9814510 / LR 0.0010000 / 86.67 seconds\n",
            "Epoch 88 of 100 / Train -1.8443509 / Val -2.0524006 / LR 0.0010000 / 86.74 seconds\n",
            "Epoch 89 of 100 / Train -1.8947908 / Val -1.7902859 / LR 0.0010000 / 85.85 seconds\n",
            "Epoch 90 of 100 / Train -1.9432403 / Val -2.2103345 / LR 0.0010000 / 86.66 seconds\n",
            "Epoch 91 of 100 / Train -1.9098164 / Val -2.1876168 / LR 0.0010000 / 86.50 seconds\n",
            "Epoch 92 of 100 / Train -1.8560770 / Val -1.6806592 / LR 0.0010000 / 86.31 seconds\n",
            "Epoch 93 of 100 / Train -1.8426593 / Val -1.8381746 / LR 0.0010000 / 85.91 seconds\n",
            "Epoch 94 of 100 / Train -1.8736639 / Val -2.2780010 / LR 0.0010000 / 84.87 seconds\n",
            "Epoch 95 of 100 / Train -1.8915573 / Val -2.3255432 / LR 0.0010000 / 84.36 seconds\n",
            "Epoch 96 of 100 / Train -1.9336747 / Val -2.0076392 / LR 0.0010000 / 84.52 seconds\n",
            "Epoch 97 of 100 / Train -1.9526312 / Val -2.3228605 / LR 0.0010000 / 84.31 seconds\n",
            "Epoch 98 of 100 / Train -1.8928675 / Val -2.1943259 / LR 0.0010000 / 84.12 seconds\n",
            "Epoch 99 of 100 / Train -1.8851954 / Val -1.9624788 / LR 0.0010000 / 84.49 seconds\n",
            "Epoch 100 of 100 / Train -1.8918315 / Val -2.3261849 / LR 0.0010000 / 84.47 seconds\n",
            "\n",
            "Total training time: 152.58 minutes\n",
            "Final train loss: -1.8918315\n",
            "Final validation loss: -2.3261849\n",
            "Model: \"hdmd\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " encoder (Sequential)        (None, 301, 2)            50178     \n",
            "                                                                 \n",
            " decoder (Sequential)        (None, 261, 2)            50178     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 100,356\n",
            "Trainable params: 100,356\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    }
  ]
}